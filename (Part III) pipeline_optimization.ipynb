{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine tuning\n",
    "\n",
    "Following the pipeline search using TPOT, the hyperparameters for each pipeline will be optimized using hyperopt. For this stage, the cost function will represent the capital return of the trading strategy.\n",
    "\n",
    "Let us recall that the desired performance of the models will not be evaluated using a common regression metric but rather by comparing the results of a trading strategy that makes use of their predictions against a buy-and-hold strategy. Therefore, for this stage of the process, the hyperparameter tuning phase will use a cost function that evaluates the performance of the trading strategy because what we actually want is for the predictions of the models to be useful in this regard.\n",
    "\n",
    "The predictions of the model will be averaged and a long (buy) position will be taken if this value is positive and a short (sell) position if it is negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from utils import *\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from hyperopt import fmin, tpe, space_eval\n",
    "from functools import partial\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vars = joblib.load('models/filtered_vars.joblib')\n",
    "cutoff_date = joblib.load('models/cutoff_date.joblib')\n",
    "df = pd.read_csv('data/req_data.csv', index_col=0, parse_dates=True).dropna()\n",
    "feats = df.drop(labels=['target'], axis=1)\n",
    "to_predict = df.loc[:, 'target']\n",
    "complete_data = pd.read_csv('data/ohlcv.csv', index_col=0, parse_dates=True)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LassoLarsCV, ElasticNetCV, SGDRegressor, RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer, KBinsDiscretizer, MaxAbsScaler, StandardScaler, Normalizer, MinMaxScaler, Binarizer, QuantileTransformer\n",
    "from sklearn.feature_selection import SelectPercentile, f_regression, SelectFwe\n",
    "from xgboost import XGBRegressor\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from tpot.builtins import StackingEstimator, ZeroCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the pipelines found in the previous step ((Part II) pipeline_search notebook). A pipeline will be optimized for each cluster phase/all data and for each data transformation (cuberoot, arcsin and none)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipelines_all = {\n",
    "    'cuberoot': make_pipeline(\n",
    "        KBinsDiscretizer(encode=\"ordinal\", n_bins=500, strategy=\"quantile\"),\n",
    "        ExtraTreesRegressor(bootstrap=True, max_features=0.5, min_samples_leaf=18, min_samples_split=8, n_estimators=100)\n",
    "    ),\n",
    "    'arsinh': make_pipeline(\n",
    "        SelectFwe(score_func=f_regression, alpha=0.048),\n",
    "        StandardScaler(),\n",
    "        GradientBoostingRegressor(alpha=0.75, learning_rate=0.001, loss=\"lad\", max_depth=9, max_features=0.2, min_samples_leaf=16, min_samples_split=18, n_estimators=100, subsample=0.4)\n",
    "    ),\n",
    "    'none': make_pipeline(\n",
    "        SelectFwe(score_func=f_regression, alpha=0.029),\n",
    "        GradientBoostingRegressor(alpha=0.99, learning_rate=0.001, loss=\"lad\", max_depth=9, max_features=0.2, min_samples_leaf=13, min_samples_split=8, n_estimators=100, subsample=0.7500000000000001)\n",
    "    )}\n",
    "\n",
    "best_pipelines_clusters = {'arsinh':{\n",
    "    '3': make_pipeline(\n",
    "        StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "        StackingEstimator(estimator=SGDRegressor(alpha=0.01, eta0=0.1, fit_intercept=True, l1_ratio=0.5, learning_rate=\"constant\", loss=\"epsilon_insensitive\", penalty=\"elasticnet\", power_t=50.0)),\n",
    "        XGBRegressor(learning_rate=0.5, max_depth=3, min_child_weight=13, n_estimators=100, n_jobs=1, objective=\"reg:squarederror\", subsample=0.9000000000000001, verbosity=0)\n",
    "    ),\n",
    "    '2': make_pipeline(\n",
    "        KBinsDiscretizer(encode=\"ordinal\", n_bins=50, strategy=\"quantile\"),\n",
    "        ExtraTreesRegressor(bootstrap=True, max_features=0.5, min_samples_leaf=4, min_samples_split=8, n_estimators=100)\n",
    "    ),\n",
    "    '1': make_pipeline(\n",
    "        QuantileTransformer(),\n",
    "        Normalizer(norm=\"l1\"),\n",
    "        StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.85, learning_rate=0.5, loss=\"quantile\", max_depth=10, max_features=0.9000000000000001, min_samples_leaf=2, min_samples_split=18, n_estimators=100, subsample=0.05)),\n",
    "        GradientBoostingRegressor(alpha=0.99, learning_rate=0.001, loss=\"lad\", max_depth=9, max_features=0.05, min_samples_leaf=15, min_samples_split=18, n_estimators=100, subsample=0.4)\n",
    "    ),\n",
    "    '0': make_pipeline(XGBRegressor(learning_rate=0.1, max_depth=1, min_child_weight=13, n_estimators=100, n_jobs=1, objective=\"reg:squarederror\", subsample=0.05, verbosity=0))    \n",
    "}, 'cuberoot': {\n",
    "    '3': make_pipeline(\n",
    "        Normalizer(norm=\"l1\"),\n",
    "        RandomForestRegressor(bootstrap=True, max_features=0.45, min_samples_leaf=8, min_samples_split=7, n_estimators=100)\n",
    "    ),\n",
    "    '2': make_pipeline(\n",
    "        Normalizer(norm=\"l1\"),\n",
    "        ExtraTreesRegressor(bootstrap=True, max_features=0.5, min_samples_leaf=4, min_samples_split=8, n_estimators=100)\n",
    "    ),\n",
    "    '1': make_pipeline(\n",
    "        StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.85, learning_rate=0.001, loss=\"quantile\", max_depth=8, max_features=0.9500000000000001, min_samples_leaf=19, min_samples_split=6, n_estimators=100, subsample=0.7000000000000001)),\n",
    "        StackingEstimator(estimator=SGDRegressor(alpha=0.01, eta0=0.01, fit_intercept=True, l1_ratio=0.0, learning_rate=\"constant\", loss=\"squared_loss\", penalty=\"elasticnet\", power_t=100.0)),\n",
    "        ExtraTreesRegressor(bootstrap=False, max_features=0.1, min_samples_leaf=9, min_samples_split=20, n_estimators=100)\n",
    "    ),\n",
    "    '0': make_pipeline(XGBRegressor(learning_rate=0.1, max_depth=5, min_child_weight=12, n_estimators=100, n_jobs=1, objective=\"reg:squarederror\", subsample=0.8500000000000001, verbosity=0)),    \n",
    "}, 'none': {\n",
    "    '3': make_pipeline(\n",
    "        StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.95, learning_rate=0.5, loss=\"ls\", max_depth=2, max_features=0.7000000000000001, min_samples_leaf=8, min_samples_split=14, n_estimators=100, subsample=0.55)),\n",
    "        Normalizer(norm=\"l1\"),\n",
    "        RandomForestRegressor(bootstrap=True, max_features=1.0, min_samples_leaf=5, min_samples_split=7, n_estimators=100)\n",
    "    ),\n",
    "    '2': make_pipeline(\n",
    "        KBinsDiscretizer(encode=\"ordinal\", n_bins=50, strategy=\"quantile\"),\n",
    "        ExtraTreesRegressor(bootstrap=True, max_features=0.5, min_samples_leaf=12, min_samples_split=8, n_estimators=100)\n",
    "    ),\n",
    "    '1': make_pipeline(ExtraTreesRegressor(bootstrap=True, max_features=0.05, min_samples_leaf=15, min_samples_split=15, n_estimators=100)),\n",
    "    '0': make_pipeline(\n",
    "        KBinsDiscretizer(encode=\"ordinal\", n_bins=500, strategy=\"uniform\"),\n",
    "        QuantileTransformer(),\n",
    "        GradientBoostingRegressor(alpha=0.9, learning_rate=0.01, loss=\"huber\", max_depth=9, max_features=0.05, min_samples_leaf=9, min_samples_split=8, n_estimators=100, subsample=0.3)\n",
    "    )    \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define broader hyperparameter search spaces for each pipeline. We will also add a roll_mean parameter which will be used to average the last n predictions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_searchspaces = {'cuberoot': {\n",
    "    'kbinsdiscretizer__n_bins': hp.choice('kbinsdiscretizer__n_bins', list(range(10, 1000, 10))),\n",
    "    'kbinsdiscretizer__strategy': hp.choice('kbinsdiscretizer__strategy', ['quantile', 'uniform']),\n",
    "    'extratreesregressor__bootstrap': hp.choice('extratreesregressor__bootstrap', [True, False]),\n",
    "    'extratreesregressor__max_features': hp.uniform('extratreesregressor__max_features', 0.05, 1),\n",
    "    'extratreesregressor__min_samples_leaf': scope.int(hp.quniform('extratreesregressor__min_samples_leaf', 2, 80, 1)),\n",
    "    'extratreesregressor__min_samples_split': scope.int(hp.quniform('extratreesregressor__min_samples_split', 2, 80, 1)),\n",
    "    'extratreesregressor__n_estimators': scope.int(hp.quniform('extratreesregressor__n_estimators', 25, 400, 1)),\n",
    "    'extratreesregressor__max_depth': scope.int(hp.quniform('extratreesregressor__max_depth', 1, 30, 1)),\n",
    "    'roll_mean': scope.int(hp.quniform('roll_mean', 1, 8, 1))\n",
    "}, 'arsinh': {\n",
    "    'gradientboostingregressor__alpha': hp.uniform('gradientboostingregressor__alpha', 0.5, 1),\n",
    "    'gradientboostingregressor__learning_rate': hp.uniform('gradientboostingregressor__learning_rate', 1e-3, 1),\n",
    "    'gradientboostingregressor__loss': hp.choice('gradientboostingregressor__loss', [\"ls\", \"lad\", \"huber\", \"quantile\"]),\n",
    "    'gradientboostingregressor__max_depth': scope.int(hp.quniform('gradientboostingregressor__max_depth', 1, 12, 1)),\n",
    "    'gradientboostingregressor__max_features': hp.uniform('gradientboostingregressor__max_features', 0.05, 1),\n",
    "    'gradientboostingregressor__min_samples_leaf': scope.int(hp.quniform('gradientboostingregressor__min_samples_leaf', 2, 80, 1)),\n",
    "    'gradientboostingregressor__min_samples_split': scope.int(hp.quniform('gradientboostingregressor__min_samples_split', 2, 80,1)),\n",
    "    'gradientboostingregressor__n_estimators': scope.int(hp.quniform('gradientboostingregressor__n_estimators', 25, 400, 1)),\n",
    "    'gradientboostingregressor__subsample': hp.uniform('gradientboostingregressor__subsample', 0.1, 1),\n",
    "    'selectfwe__alpha': hp.uniform('selectfwe__alpha', 0, 0.1),\n",
    "    'roll_mean': scope.int(hp.quniform('roll_mean', 1, 8, 1))\n",
    "}, 'none': {\n",
    "    'gradientboostingregressor__alpha': hp.uniform('gradientboostingregressor__alpha', 0.5, 1),\n",
    "    'gradientboostingregressor__learning_rate': hp.uniform('gradientboostingregressor__learning_rate', 1e-3, 1),\n",
    "    'gradientboostingregressor__loss': hp.choice('gradientboostingregressor__loss', [\"ls\", \"lad\", \"huber\", \"quantile\"]),\n",
    "    'gradientboostingregressor__max_depth': scope.int(hp.quniform('gradientboostingregressor__max_depth', 1, 12, 1)),\n",
    "    'gradientboostingregressor__max_features': hp.uniform('gradientboostingregressor__max_features', 0.05, 1),\n",
    "    'gradientboostingregressor__min_samples_leaf': scope.int(hp.quniform('gradientboostingregressor__min_samples_leaf', 2, 80, 1)),\n",
    "    'gradientboostingregressor__min_samples_split': scope.int(hp.quniform('gradientboostingregressor__min_samples_split', 2, 80, 1)),\n",
    "    'gradientboostingregressor__n_estimators': scope.int(hp.quniform('gradientboostingregressor__n_estimators', 25, 400, 1)),\n",
    "    'gradientboostingregressor__subsample': hp.uniform('gradientboostingregressor__subsample', 0.1, 1),\n",
    "    'selectfwe__alpha': hp.uniform('selectfwe__alpha', 0, 0.1),\n",
    "    'roll_mean': scope.int(hp.quniform('roll_mean', 1, 8, 1))\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_searchspaces = {\n",
    "    'cuberoot': {\n",
    "        'roll_mean': scope.int(hp.quniform('roll_mean', 1, 8, 1)),\n",
    "        '3': {\n",
    "            'randomforestregressor__bootstrap': hp.choice('randomforestregressor__bootstrap', [True, False]),\n",
    "            'randomforestregressor__max_features': hp.uniform('randomforestregressor__max_features', 0.05, 1),\n",
    "            'randomforestregressor__min_samples_leaf': scope.int(hp.quniform('randomforestregressor__min_samples_leaf', 2, 80, 1)),\n",
    "            'randomforestregressor__min_samples_split': scope.int(hp.quniform('randomforestregressor__min_samples_split', 2, 80, 1)),\n",
    "            'randomforestregressor__n_estimators': scope.int(hp.quniform('randomforestregressor__n_estimators', 25, 400, 1)),\n",
    "            'randomforestregressor__max_depth': scope.int(hp.quniform('randomforestregressor__max_depth', 1, 30, 1))   \n",
    "        },\n",
    "        '2': {\n",
    "            'extratreesregressor__bootstrap': hp.choice('2_extratreesregressor__bootstrap', [True, False]),\n",
    "            'extratreesregressor__max_features': hp.uniform('2_extratreesregressor__max_features', 0.05, 1),\n",
    "            'extratreesregressor__min_samples_leaf': scope.int(hp.quniform('2_extratreesregressor__min_samples_leaf', 2, 80, 1)),\n",
    "            'extratreesregressor__min_samples_split': scope.int(hp.quniform('2_extratreesregressor__min_samples_split', 2, 80, 1)),\n",
    "            'extratreesregressor__n_estimators': scope.int(hp.quniform('2_extratreesregressor__n_estimators', 25, 400, 1)),\n",
    "            'extratreesregressor__max_depth': scope.int(hp.quniform('2_extratreesregressor__max_depth', 1, 30, 1))\n",
    "        }, \n",
    "        '1': {\n",
    "            'stackingestimator-1__estimator__alpha': hp.uniform('stackingestimator-1__estimator__alpha', 0.5, 1),\n",
    "            'stackingestimator-1__estimator__learning_rate': hp.uniform('stackingestimator-1__estimator__learning_rate', 1e-3, 1),\n",
    "            'stackingestimator-1__estimator__loss': hp.choice('stackingestimator-1__estimator__loss', [\"ls\", \"lad\", \"huber\", \"quantile\"]),\n",
    "            'stackingestimator-1__estimator__max_depth': scope.int(hp.quniform('stackingestimator-1__estimator__max_depth', 1, 12, 1)),\n",
    "            'stackingestimator-1__estimator__max_features': hp.uniform('stackingestimator-1__estimator__max_features', 0.05, 1),\n",
    "            'stackingestimator-1__estimator__min_samples_leaf': scope.int(hp.quniform('stackingestimator-1__estimator__min_samples_leaf', 2, 80, 1)),\n",
    "            'stackingestimator-1__estimator__min_samples_split': scope.int(hp.quniform('stackingestimator-1__estimator__min_samples_split', 2, 80, 1)),\n",
    "            'stackingestimator-1__estimator__n_estimators': scope.int(hp.quniform('stackingestimator-1__estimator__n_estimators', 25, 500, 1)),\n",
    "            'stackingestimator-1__estimator__subsample': hp.uniform('stackingestimator-1__estimator__subsample', 0.1, 1),\n",
    "            'stackingestimator-2__estimator__loss': hp.choice('stackingestimator-2__estimator__loss', ['squared_loss', 'huber', 'epsilon_insensitive']),\n",
    "            'stackingestimator-2__estimator__alpha': hp.uniform('stackingestimator-2__estimator__alpha', 0, 0.01),\n",
    "            'stackingestimator-2__estimator__l1_ratio': hp.uniform('stackingestimator-2__estimator__l1_ratio', 0, 1),\n",
    "            'stackingestimator-2__estimator__eta0': hp.uniform('stackingestimator-2__estimator__eta0', 0.1, 1), \n",
    "            'stackingestimator-2__estimator__power_t': scope.int(hp.quniform('stackingestimator-2__estimator__power_t', 0, 100, 1)),\n",
    "            'extratreesregressor__bootstrap': hp.choice('extratreesregressor__bootstrap', [True, False]),\n",
    "            'extratreesregressor__max_features': hp.uniform('extratreesregressor__max_features', 0.05, 1),\n",
    "            'extratreesregressor__min_samples_leaf': scope.int(hp.quniform('extratreesregressor__min_samples_leaf', 2, 80, 1)),\n",
    "            'extratreesregressor__min_samples_split': scope.int(hp.quniform('extratreesregressor__min_samples_split', 2, 80, 1)),\n",
    "            'extratreesregressor__n_estimators': scope.int(hp.quniform('extratreesregressor__n_estimators', 25, 400, 1)),\n",
    "            'extratreesregressor__max_depth': scope.int(hp.quniform('extratreesregressor__max_depth', 1, 30, 1))\n",
    "        },\n",
    "        '0': {\n",
    "            'xgbregressor__min_child_weight': scope.int(hp.quniform('xgbregressor__min_child_weight', 2, 35, 1)),\n",
    "            'xgbregressor__reg_lambda': hp.lognormal('xgbregressor__reg_lambda', 0, 10),\n",
    "            'xgbregressor__learning_rate': hp.uniform('xgbregressor__learning_rate', 1e-3, 1),\n",
    "            'xgbregressor__subsample': hp.uniform('xgbregressor__subsample', 0.1, 1),\n",
    "            'xgbregressor__max_depth': scope.int(hp.quniform('xgbregressor__max_depth', 1, 12, 1)),\n",
    "            'xgbregressor__n_estimators': scope.int(hp.quniform('xgbregressor__n_estimators', 25, 400, 1)),\n",
    "            'xgbregressor__reg_alpha': hp.lognormal('xgbregressor__reg_alpha', 0, 10)\n",
    "        },\n",
    "    }, 'arsinh': {\n",
    "        'roll_mean': scope.int(hp.quniform('roll_mean', 1, 8, 1)),\n",
    "        '0': {\n",
    "            'xgbregressor__min_child_weight': scope.int(hp.quniform('xgbregressor__min_child_weight', 2, 35, 1)),\n",
    "            'xgbregressor__reg_lambda': hp.lognormal('xgbregressor__reg_lambda', 0, 10),\n",
    "            'xgbregressor__learning_rate': hp.uniform('xgbregressor__learning_rate', 1e-3, 1),\n",
    "            'xgbregressor__subsample': scope.int(hp.uniform('xgbregressor__subsample', 0.1, 1)),\n",
    "            'xgbregressor__max_depth': scope.int(hp.quniform('xgbregressor__max_depth', 1, 12, 1)),\n",
    "            'xgbregressor__n_estimators': scope.int(hp.quniform('xgbregressor__n_estimators', 25, 400, 1)),\n",
    "            'xgbregressor__reg_alpha': hp.lognormal('xgbregressor__reg_alpha', 0, 10)\n",
    "        },\n",
    "        '1': {\n",
    "            'gradientboostingregressor__alpha': hp.uniform('gradientboostingregressor__alpha', 0.5, 1),\n",
    "            'gradientboostingregressor__learning_rate': hp.uniform('gradientboostingregressor__learning_rate', 1e-3, 1),\n",
    "            'gradientboostingregressor__loss': hp.choice('gradientboostingregressor__loss', [\"ls\", \"lad\", \"huber\", \"quantile\"]),\n",
    "            'gradientboostingregressor__max_depth': scope.int(hp.quniform('gradientboostingregressor__max_depth', 1, 12, 1)),\n",
    "            'gradientboostingregressor__max_features': hp.uniform('gradientboostingregressor__max_features', 0.05, 1),\n",
    "            'gradientboostingregressor__min_samples_leaf': scope.int(hp.quniform('gradientboostingregressor__min_samples_leaf', 2, 80, 1)),\n",
    "            'gradientboostingregressor__min_samples_split': scope.int(hp.quniform('gradientboostingregressor__min_samples_split', 2, 80, 1)),\n",
    "            'gradientboostingregressor__n_estimators': scope.int(hp.quniform('gradientboostingregressor__n_estimators', 25, 400, 1)),\n",
    "            'gradientboostingregressor__subsample': hp.uniform('gradientboostingregressor__subsample', 0.1, 1),\n",
    "            'stackingestimator__estimator__alpha': hp.uniform('stackingestimator__estimator__alpha', 0.5, 1),\n",
    "            'stackingestimator__estimator__learning_rate': hp.uniform('stackingestimator__estimator__learning_rate', 1e-3, 1),\n",
    "            'stackingestimator__estimator__loss': hp.choice('stackingestimator__estimator__loss', [\"ls\", \"lad\", \"huber\", \"quantile\"]),\n",
    "            'stackingestimator__estimator__max_depth': scope.int(hp.quniform('stackingestimator__estimator__max_depth', 1, 12, 1)),\n",
    "            'stackingestimator__estimator__max_features': hp.uniform('stackingestimator__estimator__max_features', 0.05, 1),\n",
    "            'stackingestimator__estimator__min_samples_leaf': scope.int(hp.quniform('stackingestimator__estimator__min_samples_leaf', 2, 80, 1)),\n",
    "            'stackingestimator__estimator__min_samples_split': scope.int(hp.quniform('stackingestimator__estimator__min_samples_split', 2, 80, 1)),\n",
    "            'stackingestimator__estimator__n_estimators': scope.int(hp.quniform('stackingestimator__estimator__n_estimators', 25, 500, 1)),\n",
    "            'stackingestimator__estimator__subsample': hp.uniform('stackingestimator__estimator__subsample', 0.1, 1),\n",
    "        },\n",
    "        '2': {\n",
    "            'extratreesregressor__bootstrap': hp.choice('extratreesregressor__bootstrap', [True, False]),\n",
    "            'extratreesregressor__max_features': hp.uniform('extratreesregressor__max_features', 0.05, 1),\n",
    "            'extratreesregressor__min_samples_leaf': scope.int(hp.quniform('extratreesregressor__min_samples_leaf', 2, 80, 1)),\n",
    "            'extratreesregressor__min_samples_split': scope.int(hp.quniform('extratreesregressor__min_samples_split', 2, 80, 1)),\n",
    "            'extratreesregressor__n_estimators': scope.int(hp.quniform('extratreesregressor__n_estimators', 25, 400, 1)),\n",
    "            'extratreesregressor__max_depth': scope.int(hp.quniform('extratreesregressor__max_depth', 1, 30, 1)),\n",
    "            'kbinsdiscretizer__n_bins': hp.choice('kbinsdiscretizer__n_bins', list(range(10, 1000, 10))),\n",
    "            'kbinsdiscretizer__strategy': hp.choice('kbinsdiscretizer__strategy', ['quantile', 'uniform'])\n",
    "        },\n",
    "        '3': {\n",
    "            'xgbregressor__min_child_weight': scope.int(hp.quniform('3_xgbregressor__min_child_weight', 2, 35, 1)),\n",
    "            'xgbregressor__reg_lambda': hp.lognormal('3_xgbregressor__reg_lambda', 0, 10),\n",
    "            'xgbregressor__learning_rate': hp.uniform('3_xgbregressor__learning_rate', 1e-3, 1),\n",
    "            'xgbregressor__subsample': hp.uniform('3_xgbregressor__subsample', 0.1, 1),\n",
    "            'xgbregressor__max_depth': scope.int(hp.quniform('3_xgbregressor__max_depth', 1, 12, 1)),\n",
    "            'xgbregressor__n_estimators': scope.int(hp.quniform('3_xgbregressor__n_estimators', 25, 400, 1)),\n",
    "            'xgbregressor__reg_alpha': hp.lognormal('3_xgbregressor__reg_alpha', 0, 10),\n",
    "            'stackingestimator-2__estimator__loss': hp.choice('stackingestimator-2__estimator__loss', ['squared_loss', 'huber', 'epsilon_insensitive']),\n",
    "            'stackingestimator-2__estimator__alpha': hp.uniform('stackingestimator-2__estimator__alpha', 0, 0.01),\n",
    "            'stackingestimator-2__estimator__l1_ratio': hp.uniform('stackingestimator-2__estimator__l1_ratio', 0, 1),\n",
    "            'stackingestimator-2__estimator__eta0': hp.uniform('stackingestimator-2__estimator__eta0', 0.1, 1), \n",
    "            'stackingestimator-2__estimator__power_t': scope.int(hp.quniform('stackingestimator-2__estimator__power_t', 0, 100, 1))\n",
    "        }\n",
    "    }, 'none': {\n",
    "        'roll_mean': scope.int(hp.quniform('roll_mean', 1, 8, 1)),\n",
    "        '0': {\n",
    "            'kbinsdiscretizer__n_bins': hp.choice('kbinsdiscretizer__n_bins', list(range(10, 1000, 10))),\n",
    "            'kbinsdiscretizer__strategy': hp.choice('kbinsdiscretizer__strategy', ['quantile', 'uniform']),\n",
    "            'gradientboostingregressor__alpha': hp.uniform('gradientboostingregressor__alpha', 0.5, 1),\n",
    "            'gradientboostingregressor__learning_rate': hp.uniform('gradientboostingregressor__learning_rate', 1e-3, 1),\n",
    "            'gradientboostingregressor__loss': hp.choice('gradientboostingregressor__loss', [\"ls\", \"lad\", \"huber\", \"quantile\"]),\n",
    "            'gradientboostingregressor__max_depth': scope.int(hp.quniform('gradientboostingregressor__max_depth', 1, 12, 1)),\n",
    "            'gradientboostingregressor__max_features': hp.uniform('gradientboostingregressor__max_features', 0.05, 1),\n",
    "            'gradientboostingregressor__min_samples_leaf': scope.int(hp.quniform('gradientboostingregressor__min_samples_leaf', 2, 80, 1)),\n",
    "            'gradientboostingregressor__min_samples_split': scope.int(hp.quniform('gradientboostingregressor__min_samples_split', 2, 80, 1)),\n",
    "            'gradientboostingregressor__n_estimators': scope.int(hp.quniform('gradientboostingregressor__n_estimators', 25, 400, 1)),\n",
    "            'gradientboostingregressor__subsample': hp.uniform('gradientboostingregressor__subsample', 0.1, 1)\n",
    "        },\n",
    "        '1': {\n",
    "            'extratreesregressor__bootstrap': hp.choice('extratreesregressor__bootstrap', [True, False]),\n",
    "            'extratreesregressor__max_features': hp.uniform('extratreesregressor__max_features', 0.05, 1),\n",
    "            'extratreesregressor__min_samples_leaf': scope.int(hp.quniform('extratreesregressor__min_samples_leaf', 2, 80, 1)),\n",
    "            'extratreesregressor__min_samples_split': scope.int(hp.quniform('extratreesregressor__min_samples_split', 2, 80, 1)),\n",
    "            'extratreesregressor__n_estimators': scope.int(hp.quniform('extratreesregressor__n_estimators', 25, 400, 1)),\n",
    "            'extratreesregressor__max_depth': scope.int(hp.quniform('extratreesregressor__max_depth', 1, 30, 1))\n",
    "        },\n",
    "        '2': {\n",
    "            'kbinsdiscretizer__n_bins': hp.choice('2_kbinsdiscretizer__n_bins', list(range(10, 1000, 10))),\n",
    "            'kbinsdiscretizer__strategy': hp.choice('2_kbinsdiscretizer__strategy', ['quantile', 'uniform']),\n",
    "            'extratreesregressor__bootstrap': hp.choice('2_extratreesregressor__bootstrap', [True, False]),\n",
    "            'extratreesregressor__max_features': hp.uniform('2_extratreesregressor__max_features', 0.05, 1),\n",
    "            'extratreesregressor__min_samples_leaf': scope.int(hp.quniform('2_extratreesregressor__min_samples_leaf', 2, 80, 1)),\n",
    "            'extratreesregressor__min_samples_split': scope.int(hp.quniform('2_extratreesregressor__min_samples_split', 2, 80, 1)),\n",
    "            'extratreesregressor__n_estimators': scope.int(hp.quniform('2_extratreesregressor__n_estimators', 25, 400, 1)),\n",
    "            'extratreesregressor__max_depth': scope.int(hp.quniform('2_extratreesregressor__max_depth', 1, 30, 1)),\n",
    "        },\n",
    "        '3': {\n",
    "            'stackingestimator__estimator__alpha': hp.uniform('stackingestimator__estimator__alpha', 0.5, 1),\n",
    "            'stackingestimator__estimator__learning_rate': hp.uniform('stackingestimator__estimator__learning_rate', 1e-3, 1),\n",
    "            'stackingestimator__estimator__loss': hp.choice('stackingestimator__estimator__loss', [\"ls\", \"lad\", \"huber\", \"quantile\"]),\n",
    "            'stackingestimator__estimator__max_depth': scope.int(hp.quniform('stackingestimator__estimator__max_depth', 1, 12, 1)),\n",
    "            'stackingestimator__estimator__max_features': hp.uniform('stackingestimator__estimator__max_features', 0.05, 1),\n",
    "            'stackingestimator__estimator__min_samples_leaf': scope.int(hp.quniform('stackingestimator__estimator__min_samples_leaf', 2, 80, 1)),\n",
    "            'stackingestimator__estimator__min_samples_split': scope.int(hp.quniform('stackingestimator__estimator__min_samples_split', 2, 80, 1)),\n",
    "            'stackingestimator__estimator__n_estimators': scope.int(hp.quniform('stackingestimator__estimator__n_estimators', 25, 500, 1)),\n",
    "            'stackingestimator__estimator__subsample': hp.uniform('stackingestimator__estimator__subsample', 0.1, 1),\n",
    "            'randomforestregressor__bootstrap': hp.choice('randomforestregressor__bootstrap', [True, False]),\n",
    "            'randomforestregressor__max_features': hp.uniform('randomforestregressor__max_features', 0.05, 1),\n",
    "            'randomforestregressor__min_samples_leaf': scope.int(hp.quniform('randomforestregressor__min_samples_leaf', 2, 80, 1)),\n",
    "            'randomforestregressor__min_samples_split': scope.int(hp.quniform('randomforestregressor__min_samples_split', 2, 80, 1)),\n",
    "            'randomforestregressor__n_estimators': scope.int(hp.quniform('randomforestregressor__n_estimators', 25, 400, 1)),\n",
    "            'randomforestregressor__max_depth': scope.int(hp.quniform('randomforestregressor__max_depth', 1, 30, 1))\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_lag = ['h_high_close', 'h_low_close', 'h_candle_body', 'h_rsi_13h', 'h_ema_50', 'h_ema_200', 'h_obv10_obv50',\n",
    "              'h_obv50_obv200', 'h_close_ma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformations = {'none': [lambda x: x, lambda x: x], 'arsinh': [lambda x: np.arcsinh(x), lambda x: np.sinh(x)],\n",
    "                       'cuberoot': [lambda x: np.cbrt(x), lambda x: x**(3)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_predictions = {}\n",
    "do_not_transform = ['h_weekday', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'cluster_mode', 'd_obv10_obv50',\n",
    "                   'd_obv50_obv200', 'd_hc_15davg', 'd_lc_15davg', 'd_cb_15davg', 'd_rsi_13', 'd_ret60d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicedict(d, s):\n",
    "    return {k:v for k,v in d.items() if not k.startswith(s)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_pipeline_clusters(space, pipelines, x, y, cv, close):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Receives a pipeline and a hyperparameter space, makes predictions for each cluster and uses the predictions to predict if the next returns \n",
    "    will be positive or negative. It averages the returns and takes a long (buy) position if the average is positive and short\n",
    "    (sell) position if it is negative.\n",
    "    \n",
    "    Outputs the weekly mean return of the trading strategy.\n",
    "    \"\"\"\n",
    "    \n",
    "    for key, value in pipelines.items():\n",
    "        value.set_params(**space[key])\n",
    "        \n",
    "    preds = {key: [] for key in pipelines.keys()}\n",
    "    for train, test in cv.split(x):\n",
    "        \n",
    "        temp_x = x.iloc[train, :]\n",
    "        cluster_indices = {'0': temp_x[temp_x['cluster_mode']==0].index, '1': temp_x[temp_x['cluster_mode']==1].index,\n",
    "                           '2': temp_x[temp_x['cluster_mode']==2].index, '3': temp_x[temp_x['cluster_mode']==3].index}\n",
    "        \n",
    "        for key, value in pipelines.items():\n",
    "            value.fit(temp_x.loc[cluster_indices[key], filtered_vars[int(key)]], y.loc[cluster_indices[key]])\n",
    "            \n",
    "            to_predict = x.iloc[test].loc[:, filtered_vars[int(key)]]\n",
    "            preds[key].append(pd.Series(data=value.predict(to_predict), index=to_predict.index))\n",
    "    \n",
    "    preds = pd.concat({k: pd.concat(v, axis=0) for k, v in preds.items()}, axis=1)\n",
    "    preds = pd.concat([preds, x.loc[:, 'cluster_mode']], axis=1).dropna()\n",
    "    \n",
    "    melted = preds.melt(ignore_index=False, id_vars='cluster_mode')\n",
    "    melted = melted[melted['cluster_mode'].astype(int) == melted['variable'].astype(int)]\n",
    "    \n",
    "    preds_rets = pd.concat([melted.iloc[:, -1].rolling(space['roll_mean']).mean(), close.pct_change().shift(-1)], axis=1).dropna()\n",
    "    \n",
    "    invert_rets = preds_rets.iloc[:, 0] < 0\n",
    "    rets = preds_rets.iloc[:, 1].values\n",
    "    rets[invert_rets] = rets[invert_rets] * -1\n",
    "    \n",
    "    capital = pd.Series(data=np.cumprod(1 + rets), index=preds_rets.index)\n",
    "    weekly_mean_return = capital.resample('1W').last().pct_change().dropna().mean()\n",
    "    \n",
    "    return weekly_mean_return * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_pipeline_all(space, pipeline, x, y, cv, close):\n",
    "    \n",
    "    \"\"\"\n",
    "    Receives a pipeline and a hyperparameter space, makes predictions and uses the predictions to predict if the next returns \n",
    "    will be positive or negative. It averages the returns and takes a long (buy) position if the average is positive and short\n",
    "    (sell) position if it is negative.\n",
    "    \n",
    "    Outputs the weekly mean return of the trading strategy.\n",
    "    \"\"\"\n",
    "    \n",
    "    pipeline.set_params(**slicedict(space, 'roll'))\n",
    "    \n",
    "    preds, scores = ts_cross_val(pipeline, x, y, cv=cv)\n",
    "    \n",
    "    preds = preds.rolling(space['roll_mean']).mean().dropna()\n",
    "    preds_rets = pd.concat([preds, close.pct_change().shift(-1)], axis=1).dropna()\n",
    "    \n",
    "    invert_rets = preds_rets.iloc[:, 0] < 0\n",
    "    rets = preds_rets.iloc[:, 1].values\n",
    "    rets[invert_rets] = rets[invert_rets] * -1\n",
    "    \n",
    "    capital = pd.Series(data=np.cumprod(1 + rets), index=preds_rets.index)\n",
    "    weekly_mean_return = capital.resample('1W').last().pct_change().dropna().mean()\n",
    "    \n",
    "    return weekly_mean_return * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_byclusters():\n",
    "    global lock\n",
    "    cv = TimeSeriesSplit(n_splits=10)\n",
    "    best_spaces_clusters = {}\n",
    "    for transformation, space in cluster_searchspaces.items():\n",
    "        \n",
    "        lock.acquire()\n",
    "        x = feats.copy()\n",
    "        x.loc[:, ~x.columns.isin(do_not_transform)] = x.loc[:, ~x.columns.isin(do_not_transform)].apply(data_transformations[transformation][0], axis=1) \n",
    "        x = shift_dataset(x.copy(), lag=True, forecast=False, nlag=50, var_lags=vars_to_lag, dropna=True)\n",
    "        x = x.loc[:cutoff_date, :]    \n",
    "        y = to_predict.loc[x.index].apply(data_transformations[transformation][0])\n",
    "        lock.release()\n",
    "        \n",
    "        fmin_objective = partial(optimize_pipeline_clusters, pipelines=best_pipelines_clusters[transformation], x=x, y=y, cv=cv, close=complete_data.loc[:, 'Close'])\n",
    "        best_model = fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=150)\n",
    "        print('cluster_' + transformation)\n",
    "        print(space_eval(space, best_model))\n",
    "        best_spaces_clusters[transformation] = space_eval(space, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all():\n",
    "    global lock\n",
    "    best_spaces_all = {}\n",
    "    for transformation, space in all_searchspaces.items():\n",
    "        \n",
    "        lock.acquire()\n",
    "        x = feats.copy()\n",
    "        x.loc[:, ~x.columns.isin(do_not_transform)] = x.loc[:, ~x.columns.isin(do_not_transform)].apply(data_transformations[transformation][0]) \n",
    "        x = shift_dataset(x.copy(), lag=True, forecast=False, nlag=50, var_lags=vars_to_lag, dropna=True)\n",
    "        x = x.loc[:cutoff_date, filtered_vars['all']]\n",
    "        y = to_predict.loc[x.index].apply(data_transformations[transformation][0])\n",
    "        lock.release()\n",
    "        \n",
    "        fmin_objective = partial(optimize_pipeline_all, pipeline=best_pipelines_all[transformation], x=x, y=y, cv=10, close=complete_data.loc[:, 'Close'])\n",
    "        best_model = fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=150)\n",
    "        print('all_' + transformation)\n",
    "        print(space_eval(space, best_model))\n",
    "        best_spaces_all[transformation] = space_eval(space, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = threading.Thread(target=train_byclusters)\n",
    "f2 = threading.Thread(target=train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?trial/s, best loss=?]\n",
      "\u001b[A                                                    \n",
      "  1%|          | 1/150 [00:32<1:20:16, 32.32s/trial, best loss: -0.020611059619549923]\n",
      "\u001b[A                                                    \n",
      "  1%|1         | 2/150 [13:28<10:30:23, 255.56s/trial, best loss: -0.04085284873270213]\n",
      "\u001b[A                                                    \n",
      "  2%|2         | 3/150 [17:33<10:18:29, 252.44s/trial, best loss: -0.04085284873270213]\n",
      "\u001b[A                                                    \n",
      "  3%|2         | 4/150 [23:06<11:13:07, 276.63s/trial, best loss: -0.04256970095572999]\n",
      "\u001b[A                                                    \n",
      "  3%|3         | 5/150 [24:46<8:59:49, 223.37s/trial, best loss: -0.04256970095572999] \n",
      "\u001b[A                                                    \n",
      "  4%|4         | 6/150 [35:44<14:09:20, 353.89s/trial, best loss: -0.04256970095572999]\n",
      "\u001b[A                                                    \n",
      "  5%|4         | 7/150 [44:52<16:21:58, 412.02s/trial, best loss: -0.04256970095572999] \n",
      "\u001b[A                                                                                     \n",
      "  5%|5         | 8/150 [1:02:11<23:40:10, 600.07s/trial, best loss: -0.04734954468927006] \n",
      "\u001b[A                                                                                       \n",
      "  6%|6         | 9/150 [1:12:57<24:02:31, 613.84s/trial, best loss: -0.05587473901428463] \n",
      "\u001b[A                                                                                       \n",
      "  7%|6         | 10/150 [1:16:10<18:58:04, 487.74s/trial, best loss: -0.05587473901428463]\n",
      "\u001b[A                                                                                       \n",
      "  7%|7         | 11/150 [1:22:08<17:19:24, 448.67s/trial, best loss: -0.05587473901428463]\n",
      "\u001b[A                                                                                       \n",
      "  8%|8         | 12/150 [1:25:58<14:41:37, 383.31s/trial, best loss: -0.05587473901428463]\n",
      "\u001b[A                                                                                       \n",
      "  9%|8         | 13/150 [1:27:08<11:00:12, 289.14s/trial, best loss: -0.05587473901428463]\n",
      "\u001b[A                                                                                       \n",
      "  9%|9         | 14/150 [1:38:35<15:25:51, 408.47s/trial, best loss: -0.05587473901428463]\n",
      "\u001b[A                                                                                       \n",
      " 10%|#         | 15/150 [1:49:02<17:46:48, 474.14s/trial, best loss: -0.05587473901428463]\n",
      "\u001b[A                                                                                       \n",
      " 11%|#         | 16/150 [1:51:37<14:05:18, 378.50s/trial, best loss: -0.05587473901428463]\n",
      "\u001b[A                                                                                       \n",
      " 11%|#1        | 17/150 [1:59:27<14:59:35, 405.83s/trial, best loss: -0.05587473901428463]\n",
      "\u001b[A                                                                                       \n",
      " 12%|#2        | 18/150 [2:01:26<11:43:16, 319.67s/trial, best loss: -0.05587473901428463]\n",
      "\u001b[A                                                                                       \n",
      " 13%|#2        | 19/150 [2:06:40<11:34:29, 318.09s/trial, best loss: -0.05587473901428463]\n",
      "\u001b[A                                                                                       \n",
      " 13%|#3        | 20/150 [2:09:29<9:52:25, 273.42s/trial, best loss: -0.05587473901428463] \n",
      "\u001b[A                                                                                       \n",
      " 14%|#4        | 21/150 [2:12:07<8:33:34, 238.87s/trial, best loss: -0.05871254921351102] \n",
      "\u001b[A                                                                                       \n",
      " 15%|#4        | 22/150 [2:15:57<8:23:42, 236.11s/trial, best loss: -0.05871254921351102] \n",
      "\u001b[A                                                                                       \n",
      " 15%|#5        | 23/150 [2:35:24<18:11:04, 515.47s/trial, best loss: -0.05871254921351102]\n",
      "\u001b[A                                                                                       \n",
      " 16%|#6        | 24/150 [2:43:31<17:44:23, 506.86s/trial, best loss: -0.05871254921351102]\n",
      "\u001b[A                                                                                       \n",
      " 17%|#6        | 25/150 [2:46:03<13:53:44, 400.20s/trial, best loss: -0.05871254921351102]\n",
      "\u001b[A                                                                                       \n",
      " 17%|#7        | 26/150 [2:59:22<17:54:35, 519.96s/trial, best loss: -0.05871254921351102]\n",
      "\u001b[A                                                                                       \n",
      " 18%|#8        | 27/150 [3:26:50<29:19:58, 858.52s/trial, best loss: -0.05871254921351102] \n",
      "\u001b[A                                                                                        \n",
      " 19%|#8        | 28/150 [3:38:44<27:36:59, 814.91s/trial, best loss: -0.05871254921351102] \n",
      "\u001b[A                                                                                        \n",
      " 19%|#9        | 29/150 [3:42:22<21:22:21, 635.88s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                         \n",
      " 20%|##        | 30/150 [3:44:17<15:59:06, 479.56s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                         \n",
      " 21%|##        | 31/150 [3:47:35<13:04:01, 395.31s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                         \n",
      " 21%|##1       | 32/150 [3:54:43<13:16:37, 405.07s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                         \n",
      " 22%|##2       | 33/150 [3:55:00<9:22:52, 288.66s/trial, best loss: -0.05871254921351102]   \n",
      "\u001b[A                                                                                         \n",
      " 23%|##2       | 34/150 [3:58:33<8:34:06, 265.92s/trial, best loss: -0.05871254921351102]   \n",
      "\u001b[A                                                                                         \n",
      " 23%|##3       | 35/150 [4:01:46<7:47:38, 243.99s/trial, best loss: -0.05871254921351102]   \n",
      "\u001b[A                                                                                         \n",
      " 24%|##4       | 36/150 [4:08:42<9:21:28, 295.51s/trial, best loss: -0.05871254921351102]   \n",
      "\u001b[A                                                                                         \n",
      " 25%|##4       | 37/150 [4:23:16<14:43:54, 469.33s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                        \n",
      " 25%|##5       | 38/150 [4:26:46<12:10:43, 391.46s/trial, best loss: -0.05871254921351102] \n",
      "\u001b[A                                                                                        \n",
      " 26%|##6       | 39/150 [4:30:01<10:14:50, 332.35s/trial, best loss: -0.05871254921351102] \n",
      "\u001b[A                                                                                        \n",
      " 27%|##6       | 40/150 [4:31:50<8:06:49, 265.54s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                        \n",
      " 27%|##7       | 41/150 [4:36:29<8:09:35, 269.50s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                        \n",
      " 28%|##8       | 42/150 [4:38:57<6:59:34, 233.10s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                        \n",
      " 29%|##8       | 43/150 [4:43:30<7:17:07, 245.12s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                        \n",
      " 29%|##9       | 44/150 [4:50:03<8:31:12, 289.36s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                        \n",
      " 30%|###       | 45/150 [4:54:04<8:00:52, 274.78s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31%|###       | 46/150 [5:05:50<11:40:35, 404.19s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                         \n",
      " 31%|###1      | 47/150 [5:07:16<8:50:10, 308.84s/trial, best loss: -0.05871254921351102]   \n",
      "\u001b[A                                                                                         \n",
      " 32%|###2      | 48/150 [5:19:46<12:30:07, 441.25s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                        \n",
      " 33%|###2      | 49/150 [5:21:02<9:18:06, 331.55s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                        \n",
      " 33%|###3      | 50/150 [5:23:50<7:50:40, 282.40s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                        \n",
      " 34%|###4      | 51/150 [5:26:54<6:57:27, 253.00s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                        \n",
      " 35%|###4      | 52/150 [5:30:59<6:49:17, 250.59s/trial, best loss: -0.05871254921351102]   \n",
      "\u001b[A                                                                                         \n",
      " 35%|###5      | 53/150 [5:36:12<7:15:31, 269.39s/trial, best loss: -0.05871254921351102]   \n",
      "\u001b[A                                                                                         \n",
      " 36%|###6      | 54/150 [5:48:55<11:08:01, 417.52s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                         \n",
      " 37%|###6      | 55/150 [5:54:47<10:29:43, 397.72s/trial, best loss: -0.05871254921351102]  \n",
      "\u001b[A                                                                                         \n",
      " 37%|###7      | 56/150 [5:57:38<8:36:20, 329.58s/trial, best loss: -0.059579184504440705]  \n",
      "\u001b[A                                                                                         \n",
      " 38%|###8      | 57/150 [6:00:53<7:28:15, 289.20s/trial, best loss: -0.059579184504440705]  \n",
      "\u001b[A                                                                                        \n",
      " 39%|###8      | 58/150 [6:07:38<8:16:48, 324.01s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 39%|###9      | 59/150 [6:08:37<6:11:08, 244.71s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 40%|####      | 60/150 [6:11:12<5:26:16, 217.52s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 41%|####      | 61/150 [6:20:01<7:41:15, 310.96s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 41%|####1     | 62/150 [6:23:00<6:38:13, 271.51s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 42%|####2     | 63/150 [6:28:27<6:57:59, 288.27s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 43%|####2     | 64/150 [6:33:09<6:50:18, 286.26s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 43%|####3     | 65/150 [6:36:44<6:15:25, 265.01s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 44%|####4     | 66/150 [6:39:23<5:26:12, 233.01s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 45%|####4     | 67/150 [6:42:55<5:13:51, 226.88s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 45%|####5     | 68/150 [6:44:39<4:19:42, 190.04s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 46%|####6     | 69/150 [6:46:07<3:34:58, 159.25s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 47%|####6     | 70/150 [6:49:04<3:39:39, 164.74s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 47%|####7     | 71/150 [6:52:09<3:44:39, 170.62s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 48%|####8     | 72/150 [6:56:21<4:13:37, 195.10s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 49%|####8     | 73/150 [7:00:39<4:34:33, 213.94s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 49%|####9     | 74/150 [7:09:34<6:32:53, 310.18s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 50%|#####     | 75/150 [7:13:57<6:10:11, 296.15s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 51%|#####     | 76/150 [7:14:21<4:24:41, 214.62s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 51%|#####1    | 77/150 [7:19:45<5:00:54, 247.32s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 52%|#####2    | 78/150 [7:27:43<6:19:41, 316.41s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                       \n",
      " 53%|#####2    | 79/150 [7:45:04<10:31:57, 534.05s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                       \n",
      " 53%|#####3    | 80/150 [7:56:08<11:08:13, 572.77s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                       \n",
      " 54%|#####4    | 81/150 [7:59:35<8:52:29, 463.03s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                       \n",
      " 55%|#####4    | 82/150 [8:04:19<7:44:09, 409.55s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                       \n",
      " 55%|#####5    | 83/150 [8:07:41<6:27:35, 347.09s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                       \n",
      " 56%|#####6    | 84/150 [8:24:50<10:06:53, 551.72s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 57%|#####6    | 85/150 [8:32:58<9:36:53, 532.52s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 57%|#####7    | 86/150 [8:44:52<10:26:08, 587.01s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 58%|#####8    | 87/150 [8:55:00<10:23:12, 593.53s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                       \n",
      " 59%|#####8    | 88/150 [8:56:17<7:33:09, 438.55s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                       \n",
      " 59%|#####9    | 89/150 [9:04:39<7:45:09, 457.53s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                       \n",
      " 60%|######    | 90/150 [9:14:21<8:14:48, 494.82s/trial, best loss: -0.059579184504440705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A                                                                                       \n",
      " 61%|######    | 91/150 [9:22:21<8:02:05, 490.26s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                       \n",
      " 61%|######1   | 92/150 [9:39:20<10:27:21, 648.98s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                       \n",
      " 62%|######2   | 93/150 [9:43:54<8:29:35, 536.42s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                       \n",
      " 63%|######2   | 94/150 [9:45:37<6:19:27, 406.56s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                       \n",
      " 63%|######3   | 95/150 [9:50:22<5:39:05, 369.92s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                       \n",
      " 64%|######4   | 96/150 [9:52:41<4:30:31, 300.59s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                       \n",
      " 65%|######4   | 97/150 [9:53:37<3:20:55, 227.46s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                       \n",
      " 65%|######5   | 98/150 [9:54:34<2:32:43, 176.23s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                       \n",
      " 66%|######6   | 99/150 [10:08:03<5:11:10, 366.10s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 67%|######6   | 100/150 [10:12:01<4:33:01, 327.64s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 67%|######7   | 101/150 [10:13:06<3:23:08, 248.75s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 68%|######8   | 102/150 [10:18:05<3:31:06, 263.89s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 69%|######8   | 103/150 [10:23:08<3:35:47, 275.48s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 69%|######9   | 104/150 [10:28:38<3:43:53, 292.03s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 70%|#######   | 105/150 [10:33:35<3:40:12, 293.61s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 71%|#######   | 106/150 [10:36:24<3:07:53, 256.22s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 71%|#######1  | 107/150 [10:40:25<3:00:14, 251.49s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 72%|#######2  | 108/150 [10:40:52<2:08:52, 184.11s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 73%|#######2  | 109/150 [10:48:20<2:59:57, 263.35s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 73%|#######3  | 110/150 [10:55:05<3:23:56, 305.91s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 74%|#######4  | 111/150 [10:58:02<2:53:44, 267.29s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                         \n",
      " 75%|#######4  | 112/150 [11:09:41<4:11:15, 396.74s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                         \n",
      " 75%|#######5  | 113/150 [11:10:48<3:03:41, 297.89s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                         \n",
      " 76%|#######6  | 114/150 [11:16:28<3:06:14, 310.41s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                         \n",
      " 77%|#######6  | 115/150 [11:18:54<2:32:17, 261.08s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 77%|#######7  | 116/150 [11:20:00<1:54:48, 202.59s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 78%|#######8  | 117/150 [11:27:43<2:34:24, 280.74s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 79%|#######8  | 118/150 [11:28:40<1:53:56, 213.64s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 79%|#######9  | 119/150 [11:35:00<2:16:09, 263.53s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 80%|########  | 120/150 [11:41:51<2:33:47, 307.57s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                         \n",
      " 81%|########  | 121/150 [11:52:07<3:13:28, 400.28s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 81%|########1 | 122/150 [11:56:47<2:49:53, 364.06s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 82%|########2 | 123/150 [12:01:16<2:30:57, 335.47s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 83%|########2 | 124/150 [12:04:28<2:06:50, 292.69s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 83%|########3 | 125/150 [12:11:33<2:18:25, 332.21s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 84%|########4 | 126/150 [12:14:39<1:55:21, 288.39s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 85%|########4 | 127/150 [12:22:58<2:14:49, 351.72s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                         \n",
      " 85%|########5 | 128/150 [12:25:17<1:45:30, 287.76s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                         \n",
      " 86%|########6 | 129/150 [12:30:00<1:40:15, 286.44s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                         \n",
      " 87%|########6 | 130/150 [12:33:11<1:25:56, 257.82s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 87%|########7 | 131/150 [12:36:20<1:15:03, 237.05s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 88%|########8 | 132/150 [12:52:57<2:19:30, 465.05s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 89%|########8 | 133/150 [12:58:41<2:01:27, 428.67s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 89%|########9 | 134/150 [13:10:08<2:14:57, 506.12s/trial, best loss: -0.059579184504440705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A                                                                                        \n",
      " 90%|######### | 135/150 [13:11:09<1:33:09, 372.66s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 91%|######### | 136/150 [13:14:10<1:13:33, 315.22s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 91%|#########1| 137/150 [13:19:27<1:08:22, 315.59s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                         \n",
      " 92%|#########2| 138/150 [13:23:01<57:02, 285.23s/trial, best loss: -0.059579184504440705]  \n",
      "\u001b[A                                                                                         \n",
      " 93%|#########2| 139/150 [13:29:50<59:04, 322.26s/trial, best loss: -0.059579184504440705]  \n",
      "\u001b[A                                                                                        \n",
      " 93%|#########3| 140/150 [13:39:10<1:05:37, 393.71s/trial, best loss: -0.059579184504440705]\n",
      "\u001b[A                                                                                        \n",
      " 94%|#########3| 141/150 [13:45:09<57:28, 383.18s/trial, best loss: -0.059579184504440705]  \n",
      "\u001b[A                                                                                        \n",
      " 95%|#########4| 142/150 [13:49:47<46:53, 351.74s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 95%|#########5| 143/150 [13:52:08<33:40, 288.58s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 96%|#########6| 144/150 [13:55:34<26:22, 263.70s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 97%|#########6| 145/150 [13:57:21<18:04, 216.83s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 97%|#########7| 146/150 [13:59:33<12:45, 191.33s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 98%|#########8| 147/150 [14:03:09<09:55, 198.55s/trial, best loss: -0.059579184504440705] \n",
      "\u001b[A                                                                                        \n",
      " 99%|#########8| 148/150 [14:06:08<06:25, 192.72s/trial, best loss: -0.059579184504440705]  \n",
      "\u001b[A                                                                                         \n",
      " 99%|#########9| 149/150 [14:07:25<02:37, 157.98s/trial, best loss: -0.059579184504440705]  \n",
      "\u001b[A                                                                                         \n",
      "100%|##########| 150/150 [14:09:28<00:00, 147.74s/trial, best loss: -0.059579184504440705]  \n",
      "\u001b[A                                                                                         \n",
      "100%|##########| 150/150 [14:09:28<00:00, 339.79s/trial, best loss: -0.059579184504440705]  \n",
      "all_cuberoot                                                                                \n",
      "{'extratreesregressor__bootstrap': True, 'extratreesregressor__max_depth': 28, 'extratreesregressor__max_features': 0.5700023695679756, 'extratreesregressor__min_samples_leaf': 54, 'extratreesregressor__min_samples_split': 51, 'extratreesregressor__n_estimators': 91, 'kbinsdiscretizer__n_bins': 720, 'kbinsdiscretizer__strategy': 'quantile', 'roll_mean': 4}\n",
      "  0%|          | 0/150 [00:00<?, ?trial/s, best loss=?]                                     \n",
      "\u001b[A                                                                                         \n",
      "  1%|          | 1/150 [01:28<3:39:38, 88.44s/trial, best loss: -0.018391416996444695]      \n",
      "\u001b[A                                                                                         \n",
      "  1%|1         | 2/150 [04:38<4:52:59, 118.78s/trial, best loss: -0.023619891216229188]     \n",
      "\u001b[A                                                                                         \n",
      "  2%|2         | 3/150 [07:54<5:48:28, 142.24s/trial, best loss: -0.023619891216229188]     \n",
      "\u001b[A                                                                                        \n",
      "  3%|2         | 4/150 [33:05<22:25:05, 552.78s/trial, best loss: -0.023619891216229188]   \n",
      "\u001b[A                                                                                        \n",
      "  3%|3         | 5/150 [36:03<17:44:16, 440.39s/trial, best loss: -0.02582018469948646]    \n",
      "\u001b[A                                                                                        \n",
      "  4%|4         | 6/150 [36:55<12:56:42, 323.63s/trial, best loss: -0.03756532585214175]    \n",
      "\u001b[A                                                                                        \n",
      "  5%|4         | 7/150 [39:01<10:30:01, 264.35s/trial, best loss: -0.03756532585214175]    \n",
      "\u001b[A                                                                                        \n",
      "  5%|5         | 8/150 [45:20<11:47:18, 298.86s/trial, best loss: -0.03756532585214175]    \n",
      "\u001b[A                                                                                        \n",
      "  6%|6         | 9/150 [46:57<9:19:42, 238.17s/trial, best loss: -0.03756532585214175]     \n",
      "\u001b[A                                                                                        \n",
      "  7%|6         | 10/150 [48:17<7:25:20, 190.86s/trial, best loss: -0.03756532585214175]    \n",
      "\u001b[A                                                                                        \n",
      "  7%|7         | 11/150 [48:36<5:22:24, 139.17s/trial, best loss: -0.03756532585214175]    \n",
      "\u001b[A                                                                                        \n",
      "  8%|8         | 12/150 [49:30<4:21:32, 113.72s/trial, best loss: -0.03756532585214175]    \n",
      "\u001b[A                                                                                        \n",
      "  9%|8         | 13/150 [49:54<3:17:58, 86.70s/trial, best loss: -0.03756532585214175]     \n",
      "\u001b[A                                                                                        \n",
      "  9%|9         | 14/150 [54:45<5:35:27, 148.00s/trial, best loss: -0.03756532585214175]    \n",
      "\u001b[A                                                                                        \n",
      " 10%|#         | 15/150 [1:04:38<10:33:46, 281.67s/trial, best loss: -0.03756532585214175] \n",
      "\u001b[A                                                                                        \n",
      " 11%|#         | 16/150 [1:15:08<14:22:20, 386.12s/trial, best loss: -0.03756532585214175] \n",
      "\u001b[A                                                                                        \n",
      " 11%|#1        | 17/150 [1:15:53<10:29:03, 283.79s/trial, best loss: -0.03756532585214175] \n",
      "\u001b[A                                                                                        \n",
      " 12%|#2        | 18/150 [1:17:15<8:11:18, 223.32s/trial, best loss: -0.03756532585214175]  \n",
      "\u001b[A                                                                                        \n",
      " 13%|#2        | 19/150 [1:56:17<31:14:56, 858.75s/trial, best loss: -0.03756532585214175]  \n",
      "\u001b[A                                                                                         \n",
      " 13%|#3        | 20/150 [2:01:01<24:47:36, 686.59s/trial, best loss: -0.03756532585214175]  \n",
      "\u001b[A                                                                                         \n",
      " 14%|#4        | 21/150 [2:06:43<20:53:17, 582.92s/trial, best loss: -0.03756532585214175]  \n",
      "\u001b[A                                                                                         \n",
      " 15%|#4        | 22/150 [2:10:30<16:56:02, 476.27s/trial, best loss: -0.03756532585214175]  \n",
      "\u001b[A                                                                                         \n",
      " 15%|#5        | 23/150 [2:12:52<13:16:05, 376.11s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                         \n",
      " 16%|#6        | 24/150 [2:14:35<10:17:47, 294.19s/trial, best loss: -0.05192648632053922]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A                                                                                         \n",
      " 17%|#6        | 25/150 [2:17:23<8:53:44, 256.19s/trial, best loss: -0.05192648632053922]   \n",
      "\u001b[A                                                                                         \n",
      " 17%|#7        | 26/150 [2:20:18<7:59:04, 231.81s/trial, best loss: -0.05192648632053922]   \n",
      "\u001b[A                                                                                         \n",
      " 18%|#8        | 27/150 [2:26:04<9:05:27, 266.08s/trial, best loss: -0.05192648632053922]   \n",
      "\u001b[A                                                                                        \n",
      " 19%|#8        | 28/150 [2:32:40<10:20:10, 305.01s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 19%|#9        | 29/150 [2:49:40<17:27:35, 519.47s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 20%|##        | 30/150 [2:57:16<16:41:01, 500.51s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 21%|##        | 31/150 [3:02:51<14:54:20, 450.92s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 21%|##1       | 32/150 [3:35:01<29:19:32, 894.69s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 22%|##2       | 33/150 [3:41:43<24:16:28, 746.91s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 23%|##2       | 34/150 [3:44:50<18:39:01, 578.81s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 23%|##3       | 35/150 [3:55:29<19:03:54, 596.82s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 24%|##4       | 36/150 [4:01:13<16:30:10, 521.14s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 25%|##4       | 37/150 [4:03:39<12:49:28, 408.57s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 25%|##5       | 38/150 [4:15:37<15:35:57, 501.41s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 26%|##6       | 39/150 [4:26:57<17:06:34, 554.91s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 27%|##6       | 40/150 [4:31:36<14:25:46, 472.24s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 27%|##7       | 41/150 [4:34:31<11:35:53, 383.06s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 28%|##8       | 42/150 [4:41:41<11:54:26, 396.91s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 29%|##8       | 43/150 [4:50:56<13:12:23, 444.33s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 29%|##9       | 44/150 [4:51:33<9:29:21, 322.28s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 30%|###       | 45/150 [5:02:44<12:27:06, 426.91s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 31%|###       | 46/150 [5:08:17<11:31:16, 398.82s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 31%|###1      | 47/150 [5:16:57<12:26:57, 435.12s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 32%|###2      | 48/150 [5:17:32<8:55:37, 315.07s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 33%|###2      | 49/150 [5:18:06<6:28:37, 230.86s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 33%|###3      | 50/150 [5:23:30<7:11:17, 258.77s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 34%|###4      | 51/150 [5:25:57<6:11:39, 225.25s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 35%|###4      | 52/150 [5:28:10<5:22:39, 197.54s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 35%|###5      | 53/150 [5:31:35<5:23:01, 199.81s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 36%|###6      | 54/150 [5:36:18<5:59:35, 224.74s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 37%|###6      | 55/150 [5:37:39<4:47:23, 181.51s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 37%|###7      | 56/150 [5:38:56<3:55:24, 150.26s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 38%|###8      | 57/150 [5:40:08<3:16:26, 126.74s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 39%|###8      | 58/150 [5:41:12<2:45:21, 107.84s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 39%|###9      | 59/150 [5:43:03<2:44:55, 108.74s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 40%|####      | 60/150 [5:50:29<5:15:07, 210.08s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 41%|####      | 61/150 [5:56:17<6:12:56, 251.43s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 41%|####1     | 62/150 [5:59:42<5:48:17, 237.47s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 42%|####2     | 63/150 [6:06:30<6:58:37, 288.70s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 43%|####2     | 64/150 [6:10:44<6:38:50, 278.26s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 43%|####3     | 65/150 [6:17:52<7:37:33, 322.99s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 44%|####4     | 66/150 [6:23:48<7:46:01, 332.88s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 45%|####4     | 67/150 [6:28:47<7:26:38, 322.87s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 45%|####5     | 68/150 [6:49:47<13:45:18, 603.89s/trial, best loss: -0.05192648632053922] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A                                                                                        \n",
      " 46%|####6     | 69/150 [6:58:50<13:10:55, 585.88s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 47%|####6     | 70/150 [7:01:21<10:07:07, 455.34s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 47%|####7     | 71/150 [7:04:16<8:08:42, 371.17s/trial, best loss: -0.05192648632053922]  \n",
      "\u001b[A                                                                                        \n",
      " 48%|####8     | 72/150 [7:22:36<12:46:44, 589.80s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 49%|####8     | 73/150 [7:43:22<16:49:27, 786.59s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 49%|####9     | 74/150 [8:00:01<17:57:01, 850.28s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 50%|#####     | 75/150 [8:15:13<18:06:18, 869.05s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 51%|#####     | 76/150 [8:38:23<21:04:27, 1025.23s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 51%|#####1    | 77/150 [9:00:41<22:41:25, 1118.98s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 52%|#####2    | 78/150 [9:20:07<22:39:41, 1133.08s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                         \n",
      " 53%|#####2    | 79/150 [9:39:09<22:23:55, 1135.72s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 53%|#####3    | 80/150 [10:04:11<24:13:18, 1245.69s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 54%|#####4    | 81/150 [10:27:43<24:50:04, 1295.72s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 55%|#####4    | 82/150 [10:31:01<18:15:02, 966.22s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 55%|#####5    | 83/150 [10:36:37<14:27:54, 777.24s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 56%|#####6    | 84/150 [10:54:28<15:51:43, 865.20s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 57%|#####6    | 85/150 [10:59:50<12:40:51, 702.33s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 57%|#####7    | 86/150 [11:18:44<14:47:17, 831.83s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 58%|#####8    | 87/150 [11:30:49<13:59:48, 799.82s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 59%|#####8    | 88/150 [11:48:36<15:09:26, 880.11s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 59%|#####9    | 89/150 [12:04:31<15:17:20, 902.31s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 60%|######    | 90/150 [12:08:47<11:48:28, 708.47s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 61%|######    | 91/150 [12:14:45<9:53:24, 603.47s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 61%|######1   | 92/150 [12:16:21<7:15:59, 451.03s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 62%|######2   | 93/150 [13:02:09<18:03:21, 1140.38s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 63%|######2   | 94/150 [13:04:34<13:05:26, 841.55s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 63%|######3   | 95/150 [13:08:36<10:06:34, 661.73s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 64%|######4   | 96/150 [13:23:19<10:55:14, 728.05s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 65%|######4   | 97/150 [14:03:58<18:16:44, 1241.59s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 65%|######5   | 98/150 [14:07:36<13:29:53, 934.49s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 66%|######6   | 99/150 [14:10:03<9:53:18, 698.00s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 67%|######6   | 100/150 [14:37:10<13:33:58, 976.78s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 67%|######7   | 101/150 [14:38:53<9:43:36, 714.62s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                        \n",
      " 68%|######8   | 102/150 [14:40:07<6:58:05, 522.62s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 69%|######8   | 103/150 [14:43:07<5:28:40, 419.58s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 69%|######9   | 104/150 [14:48:09<4:54:45, 384.48s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                        \n",
      " 70%|#######   | 105/150 [15:02:31<6:35:44, 527.66s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                         \n",
      " 71%|#######   | 106/150 [15:13:03<6:49:51, 558.91s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                         \n",
      " 71%|#######1  | 107/150 [15:26:02<7:27:53, 624.97s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                         \n",
      " 72%|#######2  | 108/150 [15:49:26<10:01:02, 858.63s/trial, best loss: -0.05192648632053922]\n",
      "\u001b[A                                                                                         \n",
      " 73%|#######2  | 109/150 [15:53:59<7:46:46, 683.09s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                         \n",
      " 73%|#######3  | 110/150 [16:11:06<8:44:14, 786.36s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                         \n",
      " 74%|#######4  | 111/150 [16:21:00<7:53:35, 728.61s/trial, best loss: -0.05192648632053922] \n",
      "\u001b[A                                                                                         \n",
      " 75%|#######4  | 112/150 [16:21:18<5:26:26, 515.44s/trial, best loss: -0.0551292919218872]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A                                                                                         \n",
      " 75%|#######5  | 113/150 [16:22:56<4:00:38, 390.22s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 76%|#######6  | 114/150 [16:27:14<3:30:15, 350.44s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 77%|#######6  | 115/150 [16:30:24<2:56:25, 302.45s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 77%|#######7  | 116/150 [16:31:02<2:06:21, 222.99s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 78%|#######8  | 117/150 [16:49:56<4:32:52, 496.14s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 79%|#######8  | 118/150 [16:55:38<4:00:03, 450.11s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 79%|#######9  | 119/150 [16:55:59<2:46:03, 321.41s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 80%|########  | 120/150 [16:59:51<2:27:12, 294.42s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 81%|########  | 121/150 [17:14:54<3:50:36, 477.14s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 81%|########1 | 122/150 [17:16:23<2:48:21, 360.75s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 82%|########2 | 123/150 [17:16:40<1:55:52, 257.52s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 83%|########2 | 124/150 [17:17:49<1:27:02, 200.87s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 83%|########3 | 125/150 [17:21:48<1:28:26, 212.26s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 84%|########4 | 126/150 [17:44:10<3:40:28, 551.17s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                        \n",
      " 85%|########4 | 127/150 [17:49:31<3:04:51, 482.22s/trial, best loss: -0.0551292919218872] \n",
      "\u001b[A                                                                                        \n",
      " 85%|########5 | 128/150 [17:52:14<2:21:40, 386.37s/trial, best loss: -0.0551292919218872] \n",
      "\u001b[A                                                                                        \n",
      " 86%|########6 | 129/150 [18:07:06<3:08:24, 538.29s/trial, best loss: -0.0551292919218872] \n",
      "\u001b[A                                                                                        \n",
      " 87%|########6 | 130/150 [18:16:52<3:04:07, 552.39s/trial, best loss: -0.0551292919218872] \n",
      "\u001b[A                                                                                        \n",
      " 87%|########7 | 131/150 [18:21:17<2:27:39, 466.29s/trial, best loss: -0.0551292919218872] \n",
      "\u001b[A                                                                                        \n",
      " 88%|########8 | 132/150 [18:27:42<2:12:34, 441.91s/trial, best loss: -0.0551292919218872] \n",
      "\u001b[A                                                                                        \n",
      " 89%|########8 | 133/150 [18:27:51<1:28:23, 311.94s/trial, best loss: -0.0551292919218872] \n",
      "\u001b[A                                                                                        \n",
      " 89%|########9 | 134/150 [18:37:38<1:45:10, 394.42s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 90%|######### | 135/150 [18:38:12<1:11:36, 286.42s/trial, best loss: -0.0551292919218872]  \n",
      "\u001b[A                                                                                         \n",
      " 91%|######### | 136/150 [18:39:53<53:52, 230.91s/trial, best loss: -0.0551292919218872]    \n",
      "\u001b[A                                                                                         \n",
      " 91%|#########1| 137/150 [18:43:55<50:44, 234.21s/trial, best loss: -0.0551292919218872]    \n",
      "\u001b[A                                                                                         \n",
      " 92%|#########2| 138/150 [18:51:06<58:37, 293.12s/trial, best loss: -0.0551292919218872]    \n",
      "\u001b[A                                                                                        \n",
      " 93%|#########2| 139/150 [19:03:48<1:19:32, 433.89s/trial, best loss: -0.0551292919218872] \n",
      "\u001b[A                                                                                        \n",
      " 93%|#########3| 140/150 [19:14:41<1:23:15, 499.56s/trial, best loss: -0.0551292919218872] \n",
      "\u001b[A                                                                                        \n",
      " 94%|#########3| 141/150 [19:15:52<55:38, 370.91s/trial, best loss: -0.0551292919218872]   \n",
      "\u001b[A                                                                                        \n",
      " 95%|#########4| 142/150 [19:17:59<39:43, 297.88s/trial, best loss: -0.0551292919218872]   \n",
      "\u001b[A                                                                                        \n",
      " 95%|#########5| 143/150 [19:30:08<49:49, 427.04s/trial, best loss: -0.0551292919218872]   \n",
      "\u001b[A                                                                                        \n",
      " 96%|#########6| 144/150 [19:30:54<31:17, 312.96s/trial, best loss: -0.0551292919218872]   \n",
      "\u001b[A                                                                                        \n",
      " 97%|#########6| 145/150 [19:33:11<21:40, 260.08s/trial, best loss: -0.0551292919218872]   \n",
      "\u001b[A                                                                                        \n",
      " 97%|#########7| 146/150 [19:34:10<13:18, 199.65s/trial, best loss: -0.0551292919218872]   \n",
      "\u001b[A                                                                                        \n",
      " 98%|#########8| 147/150 [19:38:18<10:42, 214.24s/trial, best loss: -0.0551292919218872]   \n",
      "\u001b[A                                                                                        \n",
      " 99%|#########8| 148/150 [19:41:33<06:56, 208.34s/trial, best loss: -0.0551292919218872]    \n",
      "\u001b[A                                                                                         \n",
      " 99%|#########9| 149/150 [19:49:33<04:49, 289.98s/trial, best loss: -0.0551292919218872]    \n",
      "\u001b[A                                                                                        \n",
      "100%|##########| 150/150 [19:52:33<00:00, 257.07s/trial, best loss: -0.0551292919218872]   \n",
      "\u001b[A                                                                                        \n",
      "100%|##########| 150/150 [19:52:33<00:00, 477.03s/trial, best loss: -0.0551292919218872]   \n",
      "all_arsinh                                                                                 \n",
      "{'gradientboostingregressor__alpha': 0.9370232212650587, 'gradientboostingregressor__learning_rate': 0.04189021009373599, 'gradientboostingregressor__loss': 'huber', 'gradientboostingregressor__max_depth': 6, 'gradientboostingregressor__max_features': 0.3112224638324981, 'gradientboostingregressor__min_samples_leaf': 35, 'gradientboostingregressor__min_samples_split': 7, 'gradientboostingregressor__n_estimators': 26, 'gradientboostingregressor__subsample': 0.21195913484335138, 'roll_mean': 5, 'selectfwe__alpha': 0.07260318354539178}\n",
      "  0%|          | 0/150 [00:00<?, ?trial/s, best loss=?]                                    \n",
      "\u001b[A                                                                                        \n",
      "  1%|          | 1/150 [10:05<25:04:33, 605.86s/trial, best loss: -0.018124072181526545]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A                                                                                        \n",
      "  1%|1         | 2/150 [12:43<19:22:56, 471.46s/trial, best loss: -0.020473750419154765]   \n",
      "\u001b[A                                                                                        \n",
      "  2%|2         | 3/150 [15:14<15:19:19, 375.23s/trial, best loss: -0.029827869836064815]   \n",
      "\u001b[A                                                                                        \n",
      "  3%|2         | 4/150 [20:12<14:16:59, 352.19s/trial, best loss: -0.03544205020040004]    \n",
      "\u001b[A                                                                                        \n",
      "  3%|3         | 5/150 [29:17<16:30:39, 409.93s/trial, best loss: -0.03554827968060809]     \n",
      "\u001b[A                                                                                         \n",
      "  4%|4         | 6/150 [34:22<15:08:27, 378.52s/trial, best loss: -0.03554827968060809]     \n",
      "\u001b[A                                                                                        \n",
      "  5%|4         | 7/150 [1:12:12<37:34:40, 946.02s/trial, best loss: -0.041894949194424144] \n",
      "\u001b[A                                                                                      \n",
      "  5%|5         | 8/150 [1:16:18<29:01:32, 735.86s/trial, best loss: -0.041894949194424144]\n",
      "\u001b[A                                                                                      \n",
      "100%|██████████| 150/150 [35:25:31<00:00, 598.89s/trial, best loss: -0.05247069271249138]\u001b[A\n",
      "100%|██████████| 150/150 [35:25:31<00:00, 850.21s/trial, best loss: -0.05247069271249138]\n",
      "cluster_cuberoot\n",
      "{'0': {'xgbregressor__learning_rate': 0.9802569498830725, 'xgbregressor__max_depth': 7, 'xgbregressor__min_child_weight': 5, 'xgbregressor__n_estimators': 37, 'xgbregressor__reg_alpha': 4.3998350343693864e-06, 'xgbregressor__reg_lambda': 18565878.962410465, 'xgbregressor__subsample': 0.9036179379050534}, '1': {'extratreesregressor__bootstrap': True, 'extratreesregressor__max_depth': 22, 'extratreesregressor__max_features': 0.7001038397966569, 'extratreesregressor__min_samples_leaf': 57, 'extratreesregressor__min_samples_split': 30, 'extratreesregressor__n_estimators': 193, 'stackingestimator-1__estimator__alpha': 0.5146486765865822, 'stackingestimator-1__estimator__learning_rate': 0.4246918739681736, 'stackingestimator-1__estimator__loss': 'quantile', 'stackingestimator-1__estimator__max_depth': 7, 'stackingestimator-1__estimator__max_features': 0.8754081806543295, 'stackingestimator-1__estimator__min_samples_leaf': 16, 'stackingestimator-1__estimator__min_samples_split': 49, 'stackingestimator-1__estimator__n_estimators': 197, 'stackingestimator-1__estimator__subsample': 0.6573745835019558, 'stackingestimator-2__estimator__alpha': 0.00800706857244989, 'stackingestimator-2__estimator__eta0': 0.45928191378028616, 'stackingestimator-2__estimator__l1_ratio': 0.3280129701853963, 'stackingestimator-2__estimator__loss': 'epsilon_insensitive', 'stackingestimator-2__estimator__power_t': 76}, '2': {'extratreesregressor__bootstrap': True, 'extratreesregressor__max_depth': 13, 'extratreesregressor__max_features': 0.23598692146039585, 'extratreesregressor__min_samples_leaf': 58, 'extratreesregressor__min_samples_split': 27, 'extratreesregressor__n_estimators': 285}, '3': {'randomforestregressor__bootstrap': True, 'randomforestregressor__max_depth': 27, 'randomforestregressor__max_features': 0.741541341268798, 'randomforestregressor__min_samples_leaf': 43, 'randomforestregressor__min_samples_split': 46, 'randomforestregressor__n_estimators': 345}, 'roll_mean': 1}\n",
      "\n",
      "  0%|          | 0/150 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  6%|6         | 9/150 [1:32:19<31:28:16, 803.52s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      "  0%|          | 0/150 [08:45<?, ?trial/s, best loss=?]\u001b[A\n",
      "  1%|          | 1/150 [09:07<22:40:43, 547.95s/trial, best loss: -0.028530452526208148]\u001b[A\n",
      "  7%|6         | 10/150 [1:36:10<24:33:58, 631.71s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      "  1%|          | 1/150 [12:36<22:40:43, 547.95s/trial, best loss: -0.028530452526208148]\u001b[A\n",
      "  1%|▏         | 2/150 [15:41<20:37:17, 501.61s/trial, best loss: -0.028530452526208148]\u001b[A\n",
      "  7%|7         | 11/150 [1:39:45<19:33:42, 506.64s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      "  1%|▏         | 2/150 [16:11<20:37:17, 501.61s/trial, best loss: -0.028530452526208148]\u001b[A\n",
      "  2%|▏         | 3/150 [19:24<17:03:58, 417.95s/trial, best loss: -0.03185089533910211] \u001b[A\n",
      "  8%|8         | 12/150 [1:44:06<16:35:34, 432.86s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      "  2%|▏         | 3/150 [20:32<17:03:58, 417.95s/trial, best loss: -0.03185089533910211]\u001b[A\n",
      "  3%|▎         | 4/150 [26:33<17:05:22, 421.39s/trial, best loss: -0.03185089533910211]\u001b[A\n",
      "  3%|▎         | 5/150 [30:50<14:59:08, 372.06s/trial, best loss: -0.03185089533910211]\u001b[A\n",
      "  9%|8         | 13/150 [1:55:08<19:05:46, 501.80s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      "  3%|▎         | 5/150 [31:34<14:59:08, 372.06s/trial, best loss: -0.03185089533910211]\u001b[A\n",
      "  9%|9         | 14/150 [1:55:36<13:34:54, 359.52s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      "  3%|▎         | 5/150 [32:02<14:59:08, 372.06s/trial, best loss: -0.03185089533910211]\u001b[A\n",
      " 10%|#         | 15/150 [1:56:01<9:43:23, 259.28s/trial, best loss: -0.041894949194424144] \n",
      "\n",
      "  3%|▎         | 5/150 [32:27<14:59:08, 372.06s/trial, best loss: -0.03185089533910211]\u001b[A\n",
      " 11%|#         | 16/150 [1:57:29<7:44:20, 207.91s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      "  3%|▎         | 5/150 [33:55<14:59:08, 372.06s/trial, best loss: -0.03185089533910211]\u001b[A\n",
      " 11%|#1        | 17/150 [1:59:19<6:35:50, 178.58s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      "  3%|▎         | 5/150 [35:46<14:59:08, 372.06s/trial, best loss: -0.03185089533910211]\u001b[A\n",
      "  4%|▍         | 6/150 [36:20<14:22:24, 359.34s/trial, best loss: -0.03185089533910211]\u001b[A\n",
      " 12%|#2        | 18/150 [2:01:51<6:15:20, 170.61s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      "  4%|▍         | 6/150 [38:18<14:22:24, 359.34s/trial, best loss: -0.03185089533910211]\u001b[A\n",
      " 13%|#2        | 19/150 [2:06:06<7:07:15, 195.69s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      "  4%|▍         | 6/150 [42:32<14:22:24, 359.34s/trial, best loss: -0.03185089533910211]\u001b[A\n",
      "  5%|▍         | 7/150 [48:08<18:26:03, 464.08s/trial, best loss: -0.03433912890224005]\u001b[A\n",
      "  5%|▌         | 8/150 [55:09<17:47:20, 450.99s/trial, best loss: -0.03433912890224005]\u001b[A\n",
      "  6%|▌         | 9/150 [1:02:31<17:34:06, 448.56s/trial, best loss: -0.03433912890224005]\u001b[A\n",
      "  7%|▋         | 10/150 [1:11:30<18:29:56, 475.69s/trial, best loss: -0.03433912890224005]\u001b[A\n",
      " 13%|#3        | 20/150 [2:41:25<27:54:31, 772.86s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      "  7%|▋         | 10/150 [1:17:51<18:29:56, 475.69s/trial, best loss: -0.03433912890224005]\u001b[A\n",
      "  7%|▋         | 11/150 [1:21:42<19:56:06, 516.30s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      " 14%|#4        | 21/150 [2:49:46<24:45:51, 691.10s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      "  7%|▋         | 11/150 [1:26:12<19:56:06, 516.30s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      "  8%|▊         | 12/150 [1:26:44<17:19:38, 452.01s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      " 15%|#4        | 22/150 [2:51:03<18:01:37, 507.01s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      "  8%|▊         | 12/150 [1:27:29<17:19:38, 452.01s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      " 15%|#5        | 23/150 [2:52:00<13:07:26, 372.02s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      "  8%|▊         | 12/150 [1:28:26<17:19:38, 452.01s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      "  9%|▊         | 13/150 [1:30:14<14:26:33, 379.52s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      "  9%|▉         | 14/150 [1:38:53<15:55:02, 421.34s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      " 10%|█         | 15/150 [1:41:40<12:56:15, 345.00s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      " 16%|#6        | 24/150 [3:11:20<21:17:42, 608.44s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      " 10%|█         | 15/150 [1:47:46<12:56:15, 345.00s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      " 11%|█         | 16/150 [1:48:40<13:40:49, 367.53s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      " 11%|█▏        | 17/150 [1:51:01<11:03:59, 299.55s/trial, best loss: -0.03531464304710195]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17%|#6        | 25/150 [3:14:45<16:55:21, 487.37s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      " 11%|█▏        | 17/150 [1:51:11<11:03:59, 299.55s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      " 12%|█▏        | 18/150 [1:55:54<10:54:48, 297.64s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      " 13%|█▎        | 19/150 [1:58:51<9:30:53, 261.48s/trial, best loss: -0.03531464304710195] \u001b[A\n",
      " 17%|#7        | 26/150 [3:29:01<20:35:31, 597.84s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      " 13%|█▎        | 19/150 [2:05:27<9:30:53, 261.48s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      " 13%|█▎        | 20/150 [2:09:19<13:24:46, 371.43s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      " 18%|#8        | 27/150 [3:39:09<20:31:59, 600.97s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      " 13%|█▎        | 20/150 [2:15:35<13:24:46, 371.43s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      " 14%|█▍        | 21/150 [2:23:30<18:27:45, 515.24s/trial, best loss: -0.03531464304710195]\u001b[A\n",
      " 15%|█▍        | 22/150 [2:34:59<20:10:34, 567.45s/trial, best loss: -0.036322331606770716]\u001b[A\n",
      " 15%|█▌        | 23/150 [2:47:24<21:53:30, 620.56s/trial, best loss: -0.036322331606770716]\u001b[A\n",
      " 16%|█▌        | 24/150 [2:58:45<22:21:38, 638.87s/trial, best loss: -0.036322331606770716]\u001b[A\n",
      " 19%|#8        | 28/150 [4:25:50<42:44:18, 1261.13s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      " 16%|█▌        | 24/150 [3:02:17<22:21:38, 638.87s/trial, best loss: -0.036322331606770716]\u001b[A\n",
      " 17%|█▋        | 25/150 [3:09:14<22:04:54, 635.96s/trial, best loss: -0.03666976581799213] \u001b[A\n",
      " 19%|#9        | 29/150 [4:36:08<35:53:46, 1067.99s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      " 17%|█▋        | 25/150 [3:12:34<22:04:54, 635.96s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 20%|##        | 30/150 [4:39:39<27:01:45, 810.88s/trial, best loss: -0.041894949194424144] \n",
      "\n",
      " 17%|█▋        | 25/150 [3:16:05<22:04:54, 635.96s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 21%|##        | 31/150 [4:42:49<20:39:04, 624.74s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      " 17%|█▋        | 25/150 [3:19:15<22:04:54, 635.96s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 17%|█▋        | 26/150 [3:22:14<23:23:12, 678.97s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 21%|##1       | 32/150 [4:54:33<21:15:11, 648.41s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      " 17%|█▋        | 26/150 [3:30:59<23:23:12, 678.97s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 18%|█▊        | 27/150 [3:32:00<22:15:09, 651.29s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 22%|##2       | 33/150 [4:56:23<15:49:52, 487.11s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      " 18%|█▊        | 27/150 [3:32:50<22:15:09, 651.29s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 23%|##2       | 34/150 [5:02:20<14:25:56, 447.90s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      " 18%|█▊        | 27/150 [3:38:46<22:15:09, 651.29s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 23%|##3       | 35/150 [5:03:33<10:42:58, 335.47s/trial, best loss: -0.041894949194424144]\n",
      "\n",
      " 18%|█▊        | 27/150 [3:39:59<22:15:09, 651.29s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 19%|█▊        | 28/150 [3:40:22<20:33:02, 606.41s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 24%|##4       | 36/150 [5:06:40<9:12:48, 290.95s/trial, best loss: -0.04644389044164209]  \n",
      "\n",
      " 19%|█▊        | 28/150 [3:43:06<20:33:02, 606.41s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 25%|##4       | 37/150 [5:14:50<11:00:25, 350.67s/trial, best loss: -0.04644389044164209]\n",
      "\n",
      " 19%|█▊        | 28/150 [3:51:16<20:33:02, 606.41s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 25%|##5       | 38/150 [5:16:33<8:35:53, 276.37s/trial, best loss: -0.04644389044164209] \n",
      "\n",
      " 19%|█▊        | 28/150 [3:52:59<20:33:02, 606.41s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 26%|##6       | 39/150 [5:18:50<7:13:43, 234.45s/trial, best loss: -0.04644389044164209] \n",
      "\n",
      " 19%|█▊        | 28/150 [3:55:16<20:33:02, 606.41s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 19%|█▉        | 29/150 [3:57:00<24:19:55, 723.93s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 20%|██        | 30/150 [4:09:51<24:36:12, 738.11s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 27%|##6       | 40/150 [5:35:50<14:21:55, 470.14s/trial, best loss: -0.04644389044164209]\n",
      "\n",
      " 20%|██        | 30/150 [4:12:16<24:36:12, 738.11s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 27%|##7       | 41/150 [5:44:52<14:53:25, 491.79s/trial, best loss: -0.04644389044164209]\n",
      "\n",
      " 20%|██        | 30/150 [4:21:18<24:36:12, 738.11s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 28%|##8       | 42/150 [5:50:43<13:29:23, 449.66s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 20%|██        | 30/150 [4:27:10<24:36:12, 738.11s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 29%|##8       | 43/150 [5:51:31<9:47:00, 329.17s/trial, best loss: -0.046595754625560794] \n",
      "\n",
      " 20%|██        | 30/150 [4:27:58<24:36:12, 738.11s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 21%|██        | 31/150 [4:28:30<28:10:03, 852.13s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 21%|██▏       | 32/150 [4:31:08<21:06:49, 644.15s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 29%|##9       | 44/150 [5:57:21<9:52:14, 335.23s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 21%|██▏       | 32/150 [4:33:47<21:06:49, 644.15s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 30%|###       | 45/150 [6:01:00<8:45:49, 300.47s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 21%|██▏       | 32/150 [4:37:26<21:06:49, 644.15s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 22%|██▏       | 33/150 [4:38:08<18:44:59, 576.92s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 31%|###       | 46/150 [6:04:58<8:08:16, 281.69s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 22%|██▏       | 33/150 [4:41:24<18:44:59, 576.92s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 31%|###1      | 47/150 [6:06:55<6:38:50, 232.34s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 22%|██▏       | 33/150 [4:43:21<18:44:59, 576.92s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 23%|██▎       | 34/150 [4:49:40<19:41:50, 611.30s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 32%|###2      | 48/150 [6:14:41<8:33:51, 302.27s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 23%|██▎       | 34/150 [4:51:07<19:41:50, 611.30s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 33%|###2      | 49/150 [6:21:30<9:22:42, 334.29s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 23%|██▎       | 34/150 [4:57:56<19:41:50, 611.30s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 33%|###3      | 50/150 [6:22:25<6:57:31, 250.51s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 23%|██▎       | 34/150 [4:58:51<19:41:50, 611.30s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 34%|###4      | 51/150 [6:23:46<5:29:19, 199.59s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 23%|██▎       | 34/150 [5:00:12<19:41:50, 611.30s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 23%|██▎       | 35/150 [5:01:20<20:22:55, 638.05s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 35%|###4      | 52/150 [6:25:19<4:34:06, 167.82s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 23%|██▎       | 35/150 [5:01:45<20:22:55, 638.05s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 24%|██▍       | 36/150 [5:10:41<19:28:00, 614.74s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 35%|###5      | 53/150 [6:34:46<7:44:59, 287.62s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 24%|██▍       | 36/150 [5:11:13<19:28:00, 614.74s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 36%|###6      | 54/150 [6:38:55<7:21:18, 275.82s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 24%|██▍       | 36/150 [5:15:21<19:28:00, 614.74s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 25%|██▍       | 37/150 [5:15:23<16:09:54, 515.00s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 25%|██▌       | 38/150 [5:22:49<15:22:54, 494.42s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 37%|###6      | 55/150 [6:47:58<9:23:37, 355.98s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 25%|██▌       | 38/150 [5:24:24<15:22:54, 494.42s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 26%|██▌       | 39/150 [5:31:27<15:27:40, 501.45s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 37%|###7      | 56/150 [6:59:22<11:52:07, 454.55s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 26%|██▌       | 39/150 [5:35:48<15:27:40, 501.45s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 38%|###8      | 57/150 [7:00:39<8:49:06, 341.36s/trial, best loss: -0.046595754625560794] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 39/150 [5:37:06<15:27:40, 501.45s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 27%|██▋       | 40/150 [5:38:52<14:48:03, 484.39s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 27%|██▋       | 41/150 [5:41:57<11:56:55, 394.63s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 39%|###8      | 58/150 [7:05:49<8:28:57, 331.93s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 27%|██▋       | 41/150 [5:42:16<11:56:55, 394.63s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 39%|###9      | 59/150 [7:06:11<6:02:21, 238.92s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 27%|██▋       | 41/150 [5:42:37<11:56:55, 394.63s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 40%|####      | 60/150 [7:14:18<7:49:48, 313.21s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 27%|██▋       | 41/150 [5:50:44<11:56:55, 394.63s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 28%|██▊       | 42/150 [5:51:57<13:40:57, 456.09s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 41%|####      | 61/150 [7:17:54<7:01:14, 283.98s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 28%|██▊       | 42/150 [5:54:20<13:40:57, 456.09s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 41%|####1     | 62/150 [7:20:23<5:57:06, 243.49s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 28%|██▊       | 42/150 [5:56:49<13:40:57, 456.09s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 29%|██▊       | 43/150 [5:58:29<12:59:19, 437.00s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 29%|██▉       | 44/150 [6:06:42<13:21:53, 453.90s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 42%|####2     | 63/150 [7:30:33<8:32:50, 353.69s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 29%|██▉       | 44/150 [6:07:00<13:21:53, 453.90s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 43%|####2     | 64/150 [7:31:22<6:15:51, 262.23s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 29%|██▉       | 44/150 [6:07:48<13:21:53, 453.90s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 30%|███       | 45/150 [6:10:32<11:16:32, 386.59s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 43%|####3     | 65/150 [7:37:45<7:02:51, 298.49s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 30%|███       | 45/150 [6:14:12<11:16:32, 386.59s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 31%|███       | 46/150 [6:17:16<11:19:17, 391.90s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 31%|███▏      | 47/150 [6:22:15<10:24:38, 363.87s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 44%|####4     | 66/150 [7:46:19<8:28:10, 362.98s/trial, best loss: -0.046595754625560794]\n",
      "\n",
      " 31%|███▏      | 47/150 [6:22:45<10:24:38, 363.87s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 32%|███▏      | 48/150 [6:25:07<8:40:39, 306.27s/trial, best loss: -0.03666976581799213] \u001b[A\n",
      " 45%|####4     | 67/150 [7:53:58<9:02:10, 391.93s/trial, best loss: -0.04814590163725516] \n",
      "\n",
      " 32%|███▏      | 48/150 [6:30:24<8:40:39, 306.27s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 45%|####5     | 68/150 [7:56:55<7:27:29, 327.44s/trial, best loss: -0.04814590163725516]\n",
      "\n",
      " 32%|███▏      | 48/150 [6:33:21<8:40:39, 306.27s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 33%|███▎      | 49/150 [6:39:26<13:14:49, 472.18s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 46%|####6     | 69/150 [8:03:11<7:41:42, 342.01s/trial, best loss: -0.04814590163725516] \n",
      "\n",
      " 33%|███▎      | 49/150 [6:39:37<13:14:49, 472.18s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 47%|####6     | 70/150 [8:11:22<8:35:28, 386.60s/trial, best loss: -0.04814590163725516] \n",
      "\n",
      " 33%|███▎      | 49/150 [6:47:48<13:14:49, 472.18s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 33%|███▎      | 50/150 [6:48:14<13:34:50, 488.90s/trial, best loss: -0.03666976581799213]\u001b[A\n",
      " 34%|███▍      | 51/150 [6:53:48<12:10:07, 442.50s/trial, best loss: -0.03980767341262914]\u001b[A\n",
      " 35%|███▍      | 52/150 [6:57:10<10:04:44, 370.25s/trial, best loss: -0.03980767341262914]\u001b[A\n",
      " 35%|███▌      | 53/150 [7:00:20<8:31:28, 316.37s/trial, best loss: -0.04642554198337497] \u001b[A\n",
      " 47%|####7     | 71/150 [8:26:46<12:01:08, 547.70s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 35%|███▌      | 53/150 [7:03:12<8:31:28, 316.37s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 36%|███▌      | 54/150 [7:03:19<7:19:57, 274.97s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 37%|███▋      | 55/150 [7:07:01<6:50:16, 259.12s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 37%|███▋      | 56/150 [7:09:36<5:56:54, 227.82s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 38%|███▊      | 57/150 [7:11:59<5:13:55, 202.53s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 39%|███▊      | 58/150 [7:14:32<4:47:47, 187.69s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 48%|####8     | 72/150 [8:40:58<13:50:56, 639.18s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 39%|███▊      | 58/150 [7:17:24<4:47:47, 187.69s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 39%|███▉      | 59/150 [7:17:52<4:49:56, 191.18s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 40%|████      | 60/150 [7:21:55<5:10:06, 206.74s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 41%|████      | 61/150 [7:24:10<4:34:46, 185.24s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 49%|####8     | 73/150 [8:48:05<12:18:38, 575.57s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 41%|████      | 61/150 [7:24:31<4:34:46, 185.24s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 41%|████▏     | 62/150 [7:27:11<4:29:46, 183.94s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 42%|████▏     | 63/150 [7:29:58<4:19:34, 179.02s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 43%|████▎     | 64/150 [7:32:13<3:57:39, 165.81s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 43%|████▎     | 65/150 [7:35:26<4:06:15, 173.83s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 49%|####9     | 74/150 [9:02:44<14:04:05, 666.39s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 43%|████▎     | 65/150 [7:39:10<4:06:15, 173.83s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 44%|████▍     | 66/150 [7:40:06<4:48:18, 205.93s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 45%|████▍     | 67/150 [7:43:01<4:31:58, 196.61s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 45%|████▌     | 68/150 [7:46:19<4:29:16, 197.03s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 50%|#####     | 75/150 [9:11:03<12:50:24, 616.33s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 45%|████▌     | 68/150 [7:47:29<4:29:16, 197.03s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 46%|████▌     | 69/150 [7:51:19<5:07:39, 227.90s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 47%|████▋     | 70/150 [7:54:05<4:39:03, 209.29s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 51%|#####     | 76/150 [9:17:55<11:24:24, 554.92s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 47%|████▋     | 70/150 [7:54:21<4:39:03, 209.29s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 47%|████▋     | 71/150 [7:59:08<5:12:38, 237.46s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 48%|████▊     | 72/150 [8:02:30<4:54:54, 226.85s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 49%|████▊     | 73/150 [8:06:57<5:06:16, 238.66s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 49%|████▉     | 74/150 [8:09:46<4:35:57, 217.87s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 50%|█████     | 75/150 [8:12:38<4:15:00, 204.01s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 51%|#####1    | 77/150 [9:37:16<14:56:34, 736.91s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 50%|█████     | 75/150 [8:13:42<4:15:00, 204.01s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 51%|█████     | 76/150 [8:16:01<4:11:31, 203.94s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 51%|█████▏    | 77/150 [8:21:08<4:45:28, 234.63s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 52%|#####2    | 78/150 [9:48:39<14:24:52, 720.72s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 51%|█████▏    | 77/150 [8:25:05<4:45:28, 234.63s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 52%|█████▏    | 78/150 [8:29:36<6:20:15, 316.88s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 53%|█████▎    | 79/150 [8:33:52<5:53:15, 298.52s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 53%|█████▎    | 80/150 [8:40:09<6:15:31, 321.88s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 54%|█████▍    | 81/150 [8:47:10<6:44:30, 351.75s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 55%|█████▍    | 82/150 [8:52:38<6:30:26, 344.51s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 53%|#####2    | 79/150 [10:17:08<20:03:25, 1016.98s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 55%|█████▍    | 82/150 [8:53:34<6:30:26, 344.51s/trial, best loss: -0.04642554198337497]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 83/150 [8:56:17<5:42:57, 307.13s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 56%|█████▌    | 84/150 [9:00:58<5:28:55, 299.03s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 57%|█████▋    | 85/150 [9:05:58<5:24:16, 299.33s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 53%|#####3    | 80/150 [10:30:06<18:22:52, 945.32s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 57%|█████▋    | 85/150 [9:06:32<5:24:16, 299.33s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 57%|█████▋    | 86/150 [9:13:15<6:03:28, 340.75s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 54%|#####4    | 81/150 [10:36:59<15:03:34, 785.72s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 57%|█████▋    | 86/150 [9:13:25<6:03:28, 340.75s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 58%|█████▊    | 87/150 [9:15:22<4:50:16, 276.46s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 59%|█████▊    | 88/150 [9:20:12<4:49:59, 280.64s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 55%|#####4    | 82/150 [10:52:20<15:36:33, 826.37s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 59%|█████▊    | 88/150 [9:28:46<4:49:59, 280.64s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 59%|█████▉    | 89/150 [9:28:48<5:57:01, 351.18s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 60%|██████    | 90/150 [9:32:58<5:20:53, 320.89s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 61%|██████    | 91/150 [9:39:14<5:31:48, 337.43s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 55%|#####5    | 83/150 [11:04:11<14:44:07, 791.75s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 61%|██████    | 91/150 [9:40:37<5:31:48, 337.43s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 61%|██████▏   | 92/150 [9:42:32<4:45:49, 295.69s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 62%|██████▏   | 93/150 [9:46:06<4:17:38, 271.20s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 63%|██████▎   | 94/150 [9:50:28<4:10:23, 268.28s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 63%|██████▎   | 95/150 [9:56:24<4:30:08, 294.70s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 64%|██████▍   | 96/150 [10:06:06<5:42:47, 380.87s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 65%|██████▍   | 97/150 [10:10:06<4:59:00, 338.49s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 56%|#####6    | 84/150 [11:37:23<21:07:02, 1151.86s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 65%|██████▍   | 97/150 [10:13:49<4:59:00, 338.49s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 65%|██████▌   | 98/150 [10:18:36<5:38:10, 390.21s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 66%|██████▌   | 99/150 [10:28:06<6:17:24, 444.01s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 57%|#####6    | 85/150 [11:52:11<19:22:02, 1072.66s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 66%|██████▌   | 99/150 [10:28:37<6:17:24, 444.01s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 67%|██████▋   | 100/150 [10:34:55<6:01:21, 433.63s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 57%|#####7    | 86/150 [11:59:42<15:45:06, 886.04s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 67%|██████▋   | 100/150 [10:36:08<6:01:21, 433.63s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 67%|██████▋   | 101/150 [10:39:33<5:15:47, 386.67s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 68%|██████▊   | 102/150 [10:42:34<4:20:08, 325.18s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 58%|#####8    | 87/150 [12:08:42<13:41:18, 782.20s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 68%|██████▊   | 102/150 [10:45:08<4:20:08, 325.18s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 59%|#####8    | 88/150 [12:09:42<9:44:39, 565.79s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 68%|██████▊   | 102/150 [10:46:09<4:20:08, 325.18s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 69%|██████▊   | 103/150 [10:46:47<3:57:38, 303.38s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 69%|██████▉   | 104/150 [10:56:41<4:59:25, 390.56s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 59%|#####9    | 89/150 [12:23:40<10:58:11, 647.40s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 69%|██████▉   | 104/150 [11:00:06<4:59:25, 390.56s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 70%|███████   | 105/150 [11:00:48<4:20:35, 347.45s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 71%|███████   | 106/150 [11:04:03<3:41:21, 301.85s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 71%|███████▏  | 107/150 [11:17:32<5:25:14, 453.83s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 72%|███████▏  | 108/150 [11:20:17<4:17:07, 367.32s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 60%|######    | 90/150 [12:45:06<13:58:52, 838.88s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 72%|███████▏  | 108/150 [11:21:32<4:17:07, 367.32s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 73%|███████▎  | 109/150 [11:25:06<3:54:56, 343.81s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 73%|███████▎  | 110/150 [11:30:07<3:40:43, 331.09s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 74%|███████▍  | 111/150 [11:34:30<3:21:57, 310.69s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 75%|███████▍  | 112/150 [11:37:57<2:57:00, 279.48s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 75%|███████▌  | 113/150 [11:41:57<2:45:06, 267.73s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 76%|███████▌  | 114/150 [11:46:03<2:36:36, 261.02s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 61%|######    | 91/150 [13:10:35<17:08:37, 1046.06s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 76%|███████▌  | 114/150 [11:47:02<2:36:36, 261.02s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 77%|███████▋  | 115/150 [11:51:23<2:42:35, 278.73s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 61%|######1   | 92/150 [13:22:17<15:11:17, 942.72s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 77%|███████▋  | 115/150 [11:58:43<2:42:35, 278.73s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 77%|███████▋  | 116/150 [12:00:12<3:20:30, 353.83s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 78%|███████▊  | 117/150 [12:05:11<3:05:36, 337.48s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 79%|███████▊  | 118/150 [12:13:54<3:29:36, 393.01s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 79%|███████▉  | 119/150 [12:17:03<2:51:31, 331.98s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 80%|████████  | 120/150 [12:22:22<2:43:57, 327.93s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 62%|######2   | 93/150 [13:50:36<18:31:11, 1169.67s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 80%|████████  | 120/150 [12:27:02<2:43:57, 327.93s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 81%|████████  | 121/150 [12:27:14<2:33:15, 317.07s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 81%|████████▏ | 122/150 [12:29:42<2:04:23, 266.55s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 63%|######2   | 94/150 [13:58:50<15:02:26, 966.90s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 81%|████████▏ | 122/150 [12:35:16<2:04:23, 266.55s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 82%|████████▏ | 123/150 [12:37:21<2:25:50, 324.08s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 63%|######3   | 95/150 [14:07:51<12:49:03, 838.98s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 82%|████████▏ | 123/150 [12:44:17<2:25:50, 324.08s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 83%|████████▎ | 124/150 [12:45:23<2:41:00, 371.58s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 64%|######4   | 96/150 [14:10:14<9:27:16, 630.31s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 83%|████████▎ | 124/150 [12:46:40<2:41:00, 371.58s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 83%|████████▎ | 125/150 [12:47:23<2:03:24, 296.16s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 84%|████████▍ | 126/150 [12:51:39<1:53:37, 284.08s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 65%|######4   | 97/150 [14:15:41<7:56:30, 539.45s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 84%|████████▍ | 126/150 [12:52:08<1:53:37, 284.08s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 85%|████████▍ | 127/150 [12:57:24<1:55:54, 302.39s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 85%|████████▌ | 128/150 [13:03:56<2:00:42, 329.18s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 65%|######5   | 98/150 [14:28:58<8:54:30, 616.74s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 85%|████████▌ | 128/150 [13:05:25<2:00:42, 329.18s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 86%|████████▌ | 129/150 [13:11:19<2:07:11, 363.39s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 66%|######6   | 99/150 [14:36:32<8:02:38, 567.80s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 86%|████████▌ | 129/150 [13:12:58<2:07:11, 363.39s/trial, best loss: -0.04642554198337497]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 130/150 [13:14:42<1:45:04, 315.22s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 87%|████████▋ | 131/150 [13:17:30<1:25:51, 271.15s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 67%|######6   | 100/150 [14:43:30<7:15:37, 522.74s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 87%|████████▋ | 131/150 [13:19:56<1:25:51, 271.15s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 67%|######7   | 101/150 [14:45:41<5:31:06, 405.43s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 87%|████████▋ | 131/150 [13:22:08<1:25:51, 271.15s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 68%|######8   | 102/150 [14:48:45<4:31:10, 338.96s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 87%|████████▋ | 131/150 [13:25:11<1:25:51, 271.15s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 88%|████████▊ | 132/150 [13:27:48<1:52:34, 375.27s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 89%|████████▊ | 133/150 [13:32:41<1:39:18, 350.50s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 89%|████████▉ | 134/150 [13:35:41<1:19:47, 299.25s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 69%|######8   | 103/150 [15:02:40<6:21:58, 487.62s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 89%|████████▉ | 134/150 [13:39:06<1:19:47, 299.25s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 90%|█████████ | 135/150 [13:41:25<1:18:09, 312.66s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 91%|█████████ | 136/150 [13:47:33<1:16:52, 329.47s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 69%|######9   | 104/150 [15:13:11<6:46:48, 530.61s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 91%|█████████ | 136/150 [13:49:37<1:16:52, 329.47s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 70%|#######   | 105/150 [15:15:27<5:09:18, 412.41s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 91%|█████████ | 136/150 [13:51:53<1:16:52, 329.47s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 91%|█████████▏| 137/150 [13:52:37<1:09:41, 321.62s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 71%|#######   | 106/150 [15:16:59<3:51:50, 316.14s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 91%|█████████▏| 137/150 [13:53:25<1:09:41, 321.62s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 92%|█████████▏| 138/150 [13:55:56<56:57, 284.83s/trial, best loss: -0.04642554198337497]  \u001b[A\n",
      " 71%|#######1  | 107/150 [15:19:52<3:15:53, 273.34s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 92%|█████████▏| 138/150 [13:56:18<56:57, 284.83s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 93%|█████████▎| 139/150 [13:59:51<49:29, 269.98s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 72%|#######2  | 108/150 [15:24:29<3:12:06, 274.44s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 93%|█████████▎| 139/150 [14:00:55<49:29, 269.98s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 93%|█████████▎| 140/150 [14:03:16<41:46, 250.61s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 94%|█████████▍| 141/150 [14:07:31<37:45, 251.76s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 95%|█████████▍| 142/150 [14:10:45<31:16, 234.55s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 95%|█████████▌| 143/150 [14:14:23<26:46, 229.52s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 96%|█████████▌| 144/150 [14:21:02<28:01, 280.19s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 97%|█████████▋| 145/150 [14:26:03<23:52, 286.56s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 97%|█████████▋| 146/150 [14:30:00<18:06, 271.63s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 98%|█████████▊| 147/150 [14:34:29<13:32, 270.93s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 99%|█████████▊| 148/150 [14:36:48<07:42, 231.37s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 99%|█████████▉| 149/150 [14:41:13<04:01, 241.37s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      " 73%|#######2  | 109/150 [16:07:51<11:04:38, 972.65s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 99%|█████████▉| 149/150 [14:44:17<04:01, 241.37s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      "100%|██████████| 150/150 [14:44:28<00:00, 353.79s/trial, best loss: -0.04642554198337497]\u001b[A\n",
      "cluster_arsinh\n",
      "{'0': {'xgbregressor__learning_rate': 0.16058781257257088, 'xgbregressor__max_depth': 6, 'xgbregressor__min_child_weight': 2, 'xgbregressor__n_estimators': 214, 'xgbregressor__reg_alpha': 0.014105255689679444, 'xgbregressor__reg_lambda': 3.6505989722328457e-07, 'xgbregressor__subsample': 0}, '1': {'gradientboostingregressor__alpha': 0.845978637036461, 'gradientboostingregressor__learning_rate': 0.6815380281604567, 'gradientboostingregressor__loss': 'ls', 'gradientboostingregressor__max_depth': 9, 'gradientboostingregressor__max_features': 0.74371794312691, 'gradientboostingregressor__min_samples_leaf': 62, 'gradientboostingregressor__min_samples_split': 67, 'gradientboostingregressor__n_estimators': 92, 'gradientboostingregressor__subsample': 0.20883109367799846, 'stackingestimator__estimator__alpha': 0.551247090515097, 'stackingestimator__estimator__learning_rate': 0.8773161945057965, 'stackingestimator__estimator__loss': 'huber', 'stackingestimator__estimator__max_depth': 2, 'stackingestimator__estimator__max_features': 0.7001653012092475, 'stackingestimator__estimator__min_samples_leaf': 69, 'stackingestimator__estimator__min_samples_split': 5, 'stackingestimator__estimator__n_estimators': 68, 'stackingestimator__estimator__subsample': 0.5612304035809031}, '2': {'extratreesregressor__bootstrap': True, 'extratreesregressor__max_depth': 25, 'extratreesregressor__max_features': 0.5219908546257384, 'extratreesregressor__min_samples_leaf': 27, 'extratreesregressor__min_samples_split': 66, 'extratreesregressor__n_estimators': 253, 'kbinsdiscretizer__n_bins': 390, 'kbinsdiscretizer__strategy': 'quantile'}, '3': {'stackingestimator-2__estimator__alpha': 0.00823134009261331, 'stackingestimator-2__estimator__eta0': 0.14003486046635275, 'stackingestimator-2__estimator__l1_ratio': 0.9938660260698831, 'stackingestimator-2__estimator__loss': 'huber', 'stackingestimator-2__estimator__power_t': 53, 'xgbregressor__learning_rate': 0.061756015510473494, 'xgbregressor__max_depth': 8, 'xgbregressor__min_child_weight': 11, 'xgbregressor__n_estimators': 203, 'xgbregressor__reg_alpha': 1.1313558221038988e-09, 'xgbregressor__reg_lambda': 1.0840535491250761e-07, 'xgbregressor__subsample': 0.4291352880026349}, 'roll_mean': 1}\n",
      "\n",
      "  0%|          | 0/150 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  1%|          | 1/150 [05:04<12:36:57, 304.82s/trial, best loss: -0.019921510481266003]\u001b[A\n",
      " 73%|#######3  | 110/150 [16:19:58<9:59:12, 898.82s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      "  1%|          | 1/150 [11:50<12:36:57, 304.82s/trial, best loss: -0.019921510481266003]\u001b[A\n",
      "  1%|▏         | 2/150 [17:14<17:46:19, 432.29s/trial, best loss: -0.019921510481266003]\u001b[A\n",
      "  2%|▏         | 3/150 [38:10<27:44:09, 679.25s/trial, best loss: -0.019921510481266003]\u001b[A\n",
      "  3%|▎         | 4/150 [41:12<21:29:58, 530.13s/trial, best loss: -0.019921510481266003]\u001b[A\n",
      "  3%|▎         | 5/150 [50:46<21:53:17, 543.43s/trial, best loss: -0.019921510481266003]\u001b[A\n",
      " 74%|#######4  | 111/150 [17:07:53<16:09:36, 1491.70s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      "  3%|▎         | 5/150 [59:45<21:53:17, 543.43s/trial, best loss: -0.019921510481266003]\u001b[A\n",
      "  4%|▍         | 6/150 [1:01:18<22:48:03, 570.02s/trial, best loss: -0.019921510481266003]\u001b[A\n",
      " 75%|#######4  | 112/150 [17:09:59<11:25:13, 1081.94s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      "  4%|▍         | 6/150 [1:01:51<22:48:03, 570.02s/trial, best loss: -0.019921510481266003]\u001b[A\n",
      "  5%|▍         | 7/150 [1:06:10<19:19:56, 486.69s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 75%|#######5  | 113/150 [17:26:26<10:49:40, 1053.52s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      "  5%|▍         | 7/150 [1:18:18<19:19:56, 486.69s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 76%|#######6  | 114/150 [17:29:17<7:53:22, 788.96s/trial, best loss: -0.04973826813682567]  \n",
      "\n",
      "  5%|▍         | 7/150 [1:21:10<19:19:56, 486.69s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 77%|#######6  | 115/150 [17:40:48<7:22:58, 759.40s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      "  5%|▍         | 7/150 [1:32:40<19:19:56, 486.69s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 77%|#######7  | 116/150 [17:42:42<5:20:36, 565.78s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      "  5%|▍         | 7/150 [1:34:34<19:19:56, 486.69s/trial, best loss: -0.023225739365011527]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 78%|#######8  | 117/150 [17:46:50<4:18:49, 470.60s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      "  5%|▍         | 7/150 [1:38:43<19:19:56, 486.69s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 79%|#######8  | 118/150 [17:47:09<2:58:39, 334.99s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      "  5%|▍         | 7/150 [1:39:01<19:19:56, 486.69s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      "  5%|▌         | 8/150 [1:41:28<38:29:48, 975.97s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      "  6%|▌         | 9/150 [1:48:32<31:44:26, 810.40s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      "  7%|▋         | 10/150 [1:55:01<26:35:49, 683.92s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      "  7%|▋         | 11/150 [2:00:33<22:20:02, 578.43s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 79%|#######9  | 119/150 [18:10:21<5:36:55, 652.12s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      "  7%|▋         | 11/150 [2:02:14<22:20:02, 578.43s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 80%|########  | 120/150 [18:28:30<6:31:33, 783.11s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      "  7%|▋         | 11/150 [2:20:22<22:20:02, 578.43s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 81%|########  | 121/150 [18:39:54<6:04:12, 753.55s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      "  7%|▋         | 11/150 [2:31:47<22:20:02, 578.43s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 81%|########1 | 122/150 [18:47:38<5:11:01, 666.48s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      "  7%|▋         | 11/150 [2:39:30<22:20:02, 578.43s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      "  8%|▊         | 12/150 [2:47:14<47:44:02, 1245.23s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 82%|########2 | 123/150 [18:57:07<4:46:48, 637.37s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      "  8%|▊         | 12/150 [2:49:00<47:44:02, 1245.23s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 83%|########2 | 124/150 [19:00:17<3:38:01, 503.14s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      "  8%|▊         | 12/150 [2:52:10<47:44:02, 1245.23s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 83%|########3 | 125/150 [19:11:53<3:53:46, 561.07s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      "  8%|▊         | 12/150 [3:03:46<47:44:02, 1245.23s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      "  9%|▊         | 13/150 [3:04:20<44:52:39, 1179.27s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 84%|########4 | 126/150 [19:13:20<2:47:30, 418.76s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      "  9%|▊         | 13/150 [3:05:13<44:52:39, 1179.27s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      "  9%|▉         | 14/150 [3:09:10<34:28:41, 912.66s/trial, best loss: -0.023225739365011527] \u001b[A\n",
      " 10%|█         | 15/150 [3:11:34<25:34:36, 682.05s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 85%|########4 | 127/150 [19:21:16<2:47:04, 435.83s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 10%|█         | 15/150 [3:13:08<25:34:36, 682.05s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 85%|########5 | 128/150 [19:35:51<3:28:09, 567.71s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 10%|█         | 15/150 [3:27:44<25:34:36, 682.05s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 86%|########6 | 129/150 [19:37:28<2:29:15, 426.45s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 10%|█         | 15/150 [3:29:20<25:34:36, 682.05s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 87%|########6 | 130/150 [20:01:12<4:01:53, 725.66s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 10%|█         | 15/150 [3:53:04<25:34:36, 682.05s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 87%|########7 | 131/150 [20:05:34<3:05:47, 586.71s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 10%|█         | 15/150 [3:57:27<25:34:36, 682.05s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 11%|█         | 16/150 [4:01:13<51:01:45, 1370.94s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 88%|########8 | 132/150 [20:17:17<3:06:24, 621.38s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 11%|█         | 16/150 [4:09:09<51:01:45, 1370.94s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 11%|█▏        | 17/150 [4:15:00<44:37:16, 1207.80s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 89%|########8 | 133/150 [20:25:59<2:47:39, 591.76s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 11%|█▏        | 17/150 [4:17:52<44:37:16, 1207.80s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 12%|█▏        | 18/150 [4:19:56<34:15:43, 934.42s/trial, best loss: -0.023225739365011527] \u001b[A\n",
      " 89%|########9 | 134/150 [20:30:27<2:11:51, 494.50s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 12%|█▏        | 18/150 [4:22:19<34:15:43, 934.42s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 90%|######### | 135/150 [20:31:47<1:32:33, 370.26s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 12%|█▏        | 18/150 [4:23:40<34:15:43, 934.42s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 13%|█▎        | 19/150 [4:35:53<34:14:31, 941.00s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 13%|█▎        | 20/150 [4:44:03<29:05:57, 805.83s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 14%|█▍        | 21/150 [4:45:44<21:18:07, 594.48s/trial, best loss: -0.023225739365011527]\u001b[A\n",
      " 15%|█▍        | 22/150 [4:47:07<15:40:48, 441.01s/trial, best loss: -0.026216371882392824]\u001b[A\n",
      " 91%|######### | 136/150 [20:59:34<2:57:09, 759.24s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 15%|█▍        | 22/150 [4:51:26<15:40:48, 441.01s/trial, best loss: -0.026216371882392824]\u001b[A\n",
      " 15%|█▌        | 23/150 [4:52:15<14:08:43, 400.97s/trial, best loss: -0.03473485661067434] \u001b[A\n",
      " 91%|#########1| 137/150 [21:01:26<2:02:25, 565.03s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 15%|█▌        | 23/150 [4:53:18<14:08:43, 400.97s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 16%|█▌        | 24/150 [4:56:29<12:29:42, 357.01s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 17%|█▋        | 25/150 [5:01:55<12:04:22, 347.70s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 17%|█▋        | 26/150 [5:11:00<14:01:02, 406.96s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 92%|#########2| 138/150 [21:25:28<2:45:38, 828.17s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 17%|█▋        | 26/150 [5:17:21<14:01:02, 406.96s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 18%|█▊        | 27/150 [5:17:48<13:54:38, 407.14s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 93%|#########2| 139/150 [21:27:24<1:52:38, 614.40s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 18%|█▊        | 27/150 [5:19:16<13:54:38, 407.14s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 19%|█▊        | 28/150 [5:27:36<15:38:01, 461.33s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 93%|#########3| 140/150 [21:44:19<2:02:28, 734.81s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 19%|█▊        | 28/150 [5:36:12<15:38:01, 461.33s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 19%|█▉        | 29/150 [5:39:10<17:51:30, 531.32s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 94%|#########3| 141/150 [21:49:36<1:31:24, 609.36s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 19%|█▉        | 29/150 [5:41:29<17:51:30, 531.32s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 95%|#########4| 142/150 [21:51:28<1:01:20, 460.07s/trial, best loss: -0.04973826813682567]\n",
      "\n",
      " 19%|█▉        | 29/150 [5:43:20<17:51:30, 531.32s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 20%|██        | 30/150 [5:46:11<16:36:20, 498.17s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 95%|#########5| 143/150 [21:58:25<52:11, 447.31s/trial, best loss: -0.04973826813682567]  \n",
      "\n",
      " 20%|██        | 30/150 [5:50:18<16:36:20, 498.17s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 21%|██        | 31/150 [5:50:52<14:18:21, 432.79s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 96%|#########6| 144/150 [22:03:30<40:28, 404.68s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 21%|██        | 31/150 [5:55:23<14:18:21, 432.79s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 97%|#########6| 145/150 [22:11:41<35:51, 430.32s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 21%|██        | 31/150 [6:03:33<14:18:21, 432.79s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 97%|#########7| 146/150 [22:13:40<22:28, 337.18s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 21%|██        | 31/150 [6:05:33<14:18:21, 432.79s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 21%|██▏       | 32/150 [6:06:37<19:13:43, 586.64s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 98%|#########8| 147/150 [22:17:08<14:55, 298.35s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 21%|██▏       | 32/150 [6:09:01<19:13:43, 586.64s/trial, best loss: -0.03473485661067434]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 33/150 [6:10:39<15:42:21, 483.26s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 99%|#########8| 148/150 [22:18:58<08:03, 241.79s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 22%|██▏       | 33/150 [6:10:51<15:42:21, 483.26s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 99%|#########9| 149/150 [22:20:15<03:12, 192.45s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 22%|██▏       | 33/150 [6:12:08<15:42:21, 483.26s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      " 23%|██▎       | 34/150 [6:18:01<15:10:27, 470.92s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      "100%|##########| 150/150 [22:27:18<00:00, 261.57s/trial, best loss: -0.04973826813682567] \n",
      "\n",
      " 23%|██▎       | 34/150 [6:19:11<15:10:27, 470.92s/trial, best loss: -0.03473485661067434]\u001b[A\n",
      "100%|##########| 150/150 [22:27:18<00:00, 538.93s/trial, best loss: -0.04973826813682567] \n",
      "all_none                                                                                  \n",
      "{'gradientboostingregressor__alpha': 0.7553060976421809, 'gradientboostingregressor__learning_rate': 0.2839323749428636, 'gradientboostingregressor__loss': 'huber', 'gradientboostingregressor__max_depth': 10, 'gradientboostingregressor__max_features': 0.26080007177634584, 'gradientboostingregressor__min_samples_leaf': 60, 'gradientboostingregressor__min_samples_split': 17, 'gradientboostingregressor__n_estimators': 381, 'gradientboostingregressor__subsample': 0.8338267581776144, 'roll_mean': 2, 'selectfwe__alpha': 0.015479514163984265}\n",
      "100%|██████████| 150/150 [21:44:32<00:00, 521.81s/trial, best loss: -0.04092732859318259]    \n",
      "cluster_none\n",
      "{'0': {'gradientboostingregressor__alpha': 0.5817632567531272, 'gradientboostingregressor__learning_rate': 0.18688149351416955, 'gradientboostingregressor__loss': 'huber', 'gradientboostingregressor__max_depth': 1, 'gradientboostingregressor__max_features': 0.5869665881877083, 'gradientboostingregressor__min_samples_leaf': 33, 'gradientboostingregressor__min_samples_split': 46, 'gradientboostingregressor__n_estimators': 87, 'gradientboostingregressor__subsample': 0.4285299877887999, 'kbinsdiscretizer__n_bins': 260, 'kbinsdiscretizer__strategy': 'uniform'}, '1': {'extratreesregressor__bootstrap': False, 'extratreesregressor__max_depth': 22, 'extratreesregressor__max_features': 0.3900812656678244, 'extratreesregressor__min_samples_leaf': 11, 'extratreesregressor__min_samples_split': 27, 'extratreesregressor__n_estimators': 361}, '2': {'extratreesregressor__bootstrap': True, 'extratreesregressor__max_depth': 14, 'extratreesregressor__max_features': 0.6575456771952172, 'extratreesregressor__min_samples_leaf': 12, 'extratreesregressor__min_samples_split': 40, 'extratreesregressor__n_estimators': 389, 'kbinsdiscretizer__n_bins': 790, 'kbinsdiscretizer__strategy': 'quantile'}, '3': {'randomforestregressor__bootstrap': False, 'randomforestregressor__max_depth': 10, 'randomforestregressor__max_features': 0.10620987698601198, 'randomforestregressor__min_samples_leaf': 49, 'randomforestregressor__min_samples_split': 15, 'randomforestregressor__n_estimators': 261, 'stackingestimator__estimator__alpha': 0.7426861881178207, 'stackingestimator__estimator__learning_rate': 0.36543828911890164, 'stackingestimator__estimator__loss': 'quantile', 'stackingestimator__estimator__max_depth': 4, 'stackingestimator__estimator__max_features': 0.7165788758137186, 'stackingestimator__estimator__min_samples_leaf': 37, 'stackingestimator__estimator__min_samples_split': 36, 'stackingestimator__estimator__n_estimators': 346, 'stackingestimator__estimator__subsample': 0.43857423516757926}, 'roll_mean': 1}\n"
     ]
    }
   ],
   "source": [
    "f1.start()\n",
    "f2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
