{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Out-of-sample predictions\n",
    "\n",
    "Having the optimal hyperparameters for each pipeline, we make predictions with unseen data. Given that bitcoin's prices are a constant stream of data, the models will be retrained once every day so that they are updated with the latest data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LassoLarsCV, ElasticNetCV, SGDRegressor, RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer, KBinsDiscretizer, MaxAbsScaler, StandardScaler, Normalizer, MinMaxScaler, Binarizer, QuantileTransformer\n",
    "from sklearn.feature_selection import SelectPercentile, f_regression, SelectFwe\n",
    "from xgboost import XGBRegressor\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from tpot.builtins import StackingEstimator, ZeroCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vars = joblib.load('models/filtered_vars.joblib')\n",
    "cutoff_date = joblib.load('models/cutoff_date.joblib')\n",
    "df = pd.read_csv('data/req_data.csv', index_col=0, parse_dates=True).dropna()\n",
    "feats = df.drop(labels=['target'], axis=1)\n",
    "to_predict = df.loc[:, 'target']\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipelines_all = {\n",
    "    'cuberoot': make_pipeline(\n",
    "        KBinsDiscretizer(encode=\"ordinal\", n_bins=500, strategy=\"quantile\"),\n",
    "        ExtraTreesRegressor(bootstrap=True, max_features=0.5, min_samples_leaf=18, min_samples_split=8, n_estimators=100)\n",
    "    ),\n",
    "    'arsinh': make_pipeline(\n",
    "        SelectFwe(score_func=f_regression, alpha=0.048),\n",
    "        StandardScaler(),\n",
    "        GradientBoostingRegressor(alpha=0.75, learning_rate=0.001, loss=\"lad\", max_depth=9, max_features=0.2, min_samples_leaf=16, min_samples_split=18, n_estimators=100, subsample=0.4)\n",
    "    ),\n",
    "    'none': make_pipeline(\n",
    "        SelectFwe(score_func=f_regression, alpha=0.029),\n",
    "        GradientBoostingRegressor(alpha=0.99, learning_rate=0.001, loss=\"lad\", max_depth=9, max_features=0.2, min_samples_leaf=13, min_samples_split=8, n_estimators=100, subsample=0.7500000000000001)\n",
    "    )}\n",
    "\n",
    "best_pipelines_clusters = {'arsinh':{\n",
    "    '3': make_pipeline(\n",
    "        StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "        StackingEstimator(estimator=SGDRegressor(alpha=0.01, eta0=0.1, fit_intercept=True, l1_ratio=0.5, learning_rate=\"constant\", loss=\"epsilon_insensitive\", penalty=\"elasticnet\", power_t=50.0)),\n",
    "        XGBRegressor(learning_rate=0.5, max_depth=3, min_child_weight=13, n_estimators=100, n_jobs=1, objective=\"reg:squarederror\", subsample=0.9000000000000001, verbosity=0)\n",
    "    ),\n",
    "    '2': make_pipeline(\n",
    "        KBinsDiscretizer(encode=\"ordinal\", n_bins=50, strategy=\"quantile\"),\n",
    "        ExtraTreesRegressor(bootstrap=True, max_features=0.5, min_samples_leaf=4, min_samples_split=8, n_estimators=100)\n",
    "    ),\n",
    "    '1': make_pipeline(\n",
    "        QuantileTransformer(),\n",
    "        Normalizer(norm=\"l1\"),\n",
    "        StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.85, learning_rate=0.5, loss=\"quantile\", max_depth=10, max_features=0.9000000000000001, min_samples_leaf=2, min_samples_split=18, n_estimators=100, subsample=0.05)),\n",
    "        GradientBoostingRegressor(alpha=0.99, learning_rate=0.001, loss=\"lad\", max_depth=9, max_features=0.05, min_samples_leaf=15, min_samples_split=18, n_estimators=100, subsample=0.4)\n",
    "    ),\n",
    "    '0': make_pipeline(XGBRegressor(learning_rate=0.1, max_depth=1, min_child_weight=13, n_estimators=100, n_jobs=1, objective=\"reg:squarederror\", subsample=0.05, verbosity=0))    \n",
    "}, 'cuberoot': {\n",
    "    '3': make_pipeline(\n",
    "        Normalizer(norm=\"l1\"),\n",
    "        RandomForestRegressor(bootstrap=True, max_features=0.45, min_samples_leaf=8, min_samples_split=7, n_estimators=100)\n",
    "    ),\n",
    "    '2': make_pipeline(\n",
    "        Normalizer(norm=\"l1\"),\n",
    "        ExtraTreesRegressor(bootstrap=True, max_features=0.5, min_samples_leaf=4, min_samples_split=8, n_estimators=100)\n",
    "    ),\n",
    "    '1': make_pipeline(\n",
    "        StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.85, learning_rate=0.001, loss=\"quantile\", max_depth=8, max_features=0.9500000000000001, min_samples_leaf=19, min_samples_split=6, n_estimators=100, subsample=0.7000000000000001)),\n",
    "        StackingEstimator(estimator=SGDRegressor(alpha=0.01, eta0=0.01, fit_intercept=True, l1_ratio=0.0, learning_rate=\"constant\", loss=\"squared_loss\", penalty=\"elasticnet\", power_t=100.0)),\n",
    "        ExtraTreesRegressor(bootstrap=False, max_features=0.1, min_samples_leaf=9, min_samples_split=20, n_estimators=100)\n",
    "    ),\n",
    "    '0': make_pipeline(XGBRegressor(learning_rate=0.1, max_depth=5, min_child_weight=12, n_estimators=100, n_jobs=1, objective=\"reg:squarederror\", subsample=0.8500000000000001, verbosity=0)),    \n",
    "}, 'none': {\n",
    "    '3': make_pipeline(\n",
    "        StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.95, learning_rate=0.5, loss=\"ls\", max_depth=2, max_features=0.7000000000000001, min_samples_leaf=8, min_samples_split=14, n_estimators=100, subsample=0.55)),\n",
    "        Normalizer(norm=\"l1\"),\n",
    "        RandomForestRegressor(bootstrap=True, max_features=1.0, min_samples_leaf=5, min_samples_split=7, n_estimators=100)\n",
    "    ),\n",
    "    '2': make_pipeline(\n",
    "        KBinsDiscretizer(encode=\"ordinal\", n_bins=50, strategy=\"quantile\"),\n",
    "        ExtraTreesRegressor(bootstrap=True, max_features=0.5, min_samples_leaf=12, min_samples_split=8, n_estimators=100)\n",
    "    ),\n",
    "    '1': make_pipeline(ExtraTreesRegressor(bootstrap=True, max_features=0.05, min_samples_leaf=15, min_samples_split=15, n_estimators=100)),\n",
    "    '0': make_pipeline(\n",
    "        KBinsDiscretizer(encode=\"ordinal\", n_bins=500, strategy=\"uniform\"),\n",
    "        QuantileTransformer(),\n",
    "        GradientBoostingRegressor(alpha=0.9, learning_rate=0.01, loss=\"huber\", max_depth=9, max_features=0.05, min_samples_leaf=9, min_samples_split=8, n_estimators=100, subsample=0.3)\n",
    "    )    \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the optimal hyperparameter spaces found during the previous phase (pipeline_optimization notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = {'cuberoot': {'extratreesregressor__bootstrap': True, 'extratreesregressor__max_depth': 28, \n",
    "            'extratreesregressor__max_features': 0.5700023695679756, 'extratreesregressor__min_samples_leaf': 54, \n",
    "            'extratreesregressor__min_samples_split': 51, 'extratreesregressor__n_estimators': 91, \n",
    "            'kbinsdiscretizer__n_bins': 720, 'kbinsdiscretizer__strategy': 'quantile', 'roll_mean': 4}, \n",
    "            'arsinh': {'gradientboostingregressor__alpha': 0.9370232212650587, \n",
    "            'gradientboostingregressor__learning_rate': 0.04189021009373599, \n",
    "            'gradientboostingregressor__loss': 'huber', 'gradientboostingregressor__max_depth': 6, \n",
    "            'gradientboostingregressor__max_features': 0.3112224638324981, \n",
    "            'gradientboostingregressor__min_samples_leaf': 35, 'gradientboostingregressor__min_samples_split': 7, \n",
    "            'gradientboostingregressor__n_estimators': 26, 'gradientboostingregressor__subsample': 0.21195913484335138, \n",
    "            'roll_mean': 5, 'selectfwe__alpha': 0.07260318354539178}, \n",
    "            'none': {'gradientboostingregressor__alpha': 0.7553060976421809, \n",
    "            'gradientboostingregressor__learning_rate': 0.2839323749428636, \n",
    "            'gradientboostingregressor__loss': 'huber', 'gradientboostingregressor__max_depth': 10, \n",
    "            'gradientboostingregressor__max_features': 0.26080007177634584, \n",
    "            'gradientboostingregressor__min_samples_leaf': 60, 'gradientboostingregressor__min_samples_split': 17, \n",
    "            'gradientboostingregressor__n_estimators': 381, \n",
    "            'gradientboostingregressor__subsample': 0.8338267581776144, 'roll_mean': 2, \n",
    "            'selectfwe__alpha': 0.015479514163984265}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bycluster_params = {\n",
    "    'cuberoot': {'0': {'xgbregressor__learning_rate': 0.9802569498830725, 'xgbregressor__max_depth': 7, \n",
    "                       'xgbregressor__min_child_weight': 5, 'xgbregressor__n_estimators': 37, \n",
    "                       'xgbregressor__reg_alpha': 4.3998350343693864e-06, 'xgbregressor__reg_lambda': 18565878.962410465, \n",
    "                       'xgbregressor__subsample': 0.9036179379050534}, \n",
    "                 '1': {'extratreesregressor__bootstrap': True, 'extratreesregressor__max_depth': 22, \n",
    "                       'extratreesregressor__max_features': 0.7001038397966569, 'extratreesregressor__min_samples_leaf': 57, \n",
    "                       'extratreesregressor__min_samples_split': 30, 'extratreesregressor__n_estimators': 193, \n",
    "                       'stackingestimator-1__estimator__alpha': 0.5146486765865822, \n",
    "                       'stackingestimator-1__estimator__learning_rate': 0.4246918739681736, \n",
    "                       'stackingestimator-1__estimator__loss': 'quantile', 'stackingestimator-1__estimator__max_depth': 7, \n",
    "                       'stackingestimator-1__estimator__max_features': 0.8754081806543295, \n",
    "                       'stackingestimator-1__estimator__min_samples_leaf': 16, 'stackingestimator-1__estimator__min_samples_split': 49, \n",
    "                       'stackingestimator-1__estimator__n_estimators': 197, 'stackingestimator-1__estimator__subsample': 0.6573745835019558, \n",
    "                       'stackingestimator-2__estimator__alpha': 0.00800706857244989, 'stackingestimator-2__estimator__eta0': 0.45928191378028616, \n",
    "                       'stackingestimator-2__estimator__l1_ratio': 0.3280129701853963, 'stackingestimator-2__estimator__loss': 'epsilon_insensitive', \n",
    "                       'stackingestimator-2__estimator__power_t': 76}, \n",
    "                 '2': {'extratreesregressor__bootstrap': True, 'extratreesregressor__max_depth': 13, \n",
    "                       'extratreesregressor__max_features': 0.23598692146039585, 'extratreesregressor__min_samples_leaf': 58, \n",
    "                       'extratreesregressor__min_samples_split': 27, 'extratreesregressor__n_estimators': 285}, \n",
    "                 '3': {'randomforestregressor__bootstrap': True, 'randomforestregressor__max_depth': 27, \n",
    "                       'randomforestregressor__max_features': 0.741541341268798, 'randomforestregressor__min_samples_leaf': 43, \n",
    "                       'randomforestregressor__min_samples_split': 46, 'randomforestregressor__n_estimators': 345}, \n",
    "                 'roll_mean': 1},\n",
    "    'arsinh': {'0': {'xgbregressor__learning_rate': 0.16058781257257088, 'xgbregressor__max_depth': 6, \n",
    "                     'xgbregressor__min_child_weight': 2, 'xgbregressor__n_estimators': 214, \n",
    "                     'xgbregressor__reg_alpha': 0.014105255689679444, 'xgbregressor__reg_lambda': 3.6505989722328457e-07, \n",
    "                     'xgbregressor__subsample': 0}, \n",
    "               '1': {'gradientboostingregressor__alpha': 0.845978637036461, \n",
    "                     'gradientboostingregressor__learning_rate': 0.6815380281604567, 'gradientboostingregressor__loss': 'ls', \n",
    "                     'gradientboostingregressor__max_depth': 9, 'gradientboostingregressor__max_features': 0.74371794312691, \n",
    "                     'gradientboostingregressor__min_samples_leaf': 62, 'gradientboostingregressor__min_samples_split': 67, \n",
    "                     'gradientboostingregressor__n_estimators': 92, \n",
    "                     'gradientboostingregressor__subsample': 0.20883109367799846, 'stackingestimator__estimator__alpha': 0.551247090515097, \n",
    "                     'stackingestimator__estimator__learning_rate': 0.8773161945057965, 'stackingestimator__estimator__loss': 'huber', \n",
    "                     'stackingestimator__estimator__max_depth': 2, 'stackingestimator__estimator__max_features': 0.7001653012092475, \n",
    "                     'stackingestimator__estimator__min_samples_leaf': 69, 'stackingestimator__estimator__min_samples_split': 5, \n",
    "                     'stackingestimator__estimator__n_estimators': 68, 'stackingestimator__estimator__subsample': 0.5612304035809031}, \n",
    "               '2': {'extratreesregressor__bootstrap': True, 'extratreesregressor__max_depth': 25, \n",
    "                     'extratreesregressor__max_features': 0.5219908546257384, 'extratreesregressor__min_samples_leaf': 27, \n",
    "                     'extratreesregressor__min_samples_split': 66, 'extratreesregressor__n_estimators': 253, \n",
    "                     'kbinsdiscretizer__n_bins': 390, 'kbinsdiscretizer__strategy': 'quantile'}, \n",
    "               '3': {'stackingestimator-2__estimator__alpha': 0.00823134009261331, 'stackingestimator-2__estimator__eta0': 0.14003486046635275, \n",
    "                     'stackingestimator-2__estimator__l1_ratio': 0.9938660260698831, 'stackingestimator-2__estimator__loss': 'huber', \n",
    "                     'stackingestimator-2__estimator__power_t': 53, 'xgbregressor__learning_rate': 0.061756015510473494, \n",
    "                     'xgbregressor__max_depth': 8, 'xgbregressor__min_child_weight': 11, 'xgbregressor__n_estimators': 203, \n",
    "                     'xgbregressor__reg_alpha': 1.1313558221038988e-09, 'xgbregressor__reg_lambda': 1.0840535491250761e-07, \n",
    "                     'xgbregressor__subsample': 0.4291352880026349}, \n",
    "               'roll_mean': 1},\n",
    "    'none': {'0': {'gradientboostingregressor__alpha': 0.5817632567531272, \n",
    "                   'gradientboostingregressor__learning_rate': 0.18688149351416955, \n",
    "                   'gradientboostingregressor__loss': 'huber', 'gradientboostingregressor__max_depth': 1, \n",
    "                   'gradientboostingregressor__max_features': 0.5869665881877083, \n",
    "                   'gradientboostingregressor__min_samples_leaf': 33, 'gradientboostingregressor__min_samples_split': 46, \n",
    "                   'gradientboostingregressor__n_estimators': 87, 'gradientboostingregressor__subsample': 0.4285299877887999, \n",
    "                   'kbinsdiscretizer__n_bins': 260, 'kbinsdiscretizer__strategy': 'uniform'}, \n",
    "             '1': {'extratreesregressor__bootstrap': False, 'extratreesregressor__max_depth': 22, \n",
    "                   'extratreesregressor__max_features': 0.3900812656678244, 'extratreesregressor__min_samples_leaf': 11, \n",
    "                   'extratreesregressor__min_samples_split': 27, 'extratreesregressor__n_estimators': 361}, \n",
    "             '2': {'extratreesregressor__bootstrap': True, 'extratreesregressor__max_depth': 14, \n",
    "                   'extratreesregressor__max_features': 0.6575456771952172, 'extratreesregressor__min_samples_leaf': 12, \n",
    "                   'extratreesregressor__min_samples_split': 40, 'extratreesregressor__n_estimators': 389, \n",
    "                   'kbinsdiscretizer__n_bins': 790, 'kbinsdiscretizer__strategy': 'quantile'}, \n",
    "             '3': {'randomforestregressor__bootstrap': False, 'randomforestregressor__max_depth': 10, \n",
    "                   'randomforestregressor__max_features': 0.10620987698601198, 'randomforestregressor__min_samples_leaf': 49, \n",
    "                   'randomforestregressor__min_samples_split': 15, 'randomforestregressor__n_estimators': 261, \n",
    "                   'stackingestimator__estimator__alpha': 0.7426861881178207, 'stackingestimator__estimator__learning_rate': 0.36543828911890164, \n",
    "                   'stackingestimator__estimator__loss': 'quantile', 'stackingestimator__estimator__max_depth': 4, \n",
    "                   'stackingestimator__estimator__max_features': 0.7165788758137186, 'stackingestimator__estimator__min_samples_leaf': 37, \n",
    "                   'stackingestimator__estimator__min_samples_split': 36, 'stackingestimator__estimator__n_estimators': 346, \n",
    "                   'stackingestimator__estimator__subsample': 0.43857423516757926}, \n",
    "            'roll_mean': 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicedict(d, s):\n",
    "    return {k:v for k,v in d.items() if not k.startswith(s)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimal hyperparameters of each pipeline\n",
    "for key in best_pipelines_all:\n",
    "    best_pipelines_all[key].set_params(**slicedict(all_params[key], 'roll_'))\n",
    "    \n",
    "for k1, v1 in best_pipelines_clusters.items():\n",
    "    for k2, v2 in v1.items():\n",
    "        best_pipelines_clusters[k1][k2].set_params(**bycluster_params[k1][k2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_lag = ['h_high_close', 'h_low_close', 'h_candle_body', 'h_rsi_13h', 'h_ema_50', 'h_ema_200', 'h_obv10_obv50',\n",
    "              'h_obv50_obv200', 'h_close_ma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformations = {'none': [lambda x: x, lambda x: x], 'arsinh': [lambda x: np.arcsinh(x), lambda x: np.sinh(x)],\n",
    "                       'cuberoot': [lambda x: np.cbrt(x), lambda x: x**(3)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_predictions = {}\n",
    "do_not_transform = ['h_weekday', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'cluster_mode', 'd_obv10_obv50',\n",
    "                   'd_obv50_obv200', 'd_hc_15davg', 'd_lc_15davg', 'd_cb_15davg', 'd_rsi_13', 'd_ret60d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated previously, given that bitcoin's price fluctuates constantly, it is a good idea to fit the model with new data as it comes. We will create two functions that will help us with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_fit_predict_clusters(x, y, pipelines, cutoff_date, refit_freq, gap=7, export_model=True, model_name=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fits and predicts in a rolling basis\n",
    "    \"\"\"\n",
    "    \n",
    "    n_freq = {'D': 24, 'W': 24*7, 'M': 24*30}\n",
    "    refit_dates = pd.date_range(cutoff_date, x.index[-1], freq=refit_freq)\n",
    "    fit_ix_iter = zip(list(map(lambda z: range(x.index.get_loc(z)), refit_dates)), \n",
    "                      list(map(lambda z: range(x.index.get_loc(z) + gap, x.index.get_loc(z) + n_freq[refit_freq] + gap), refit_dates)))\n",
    "    \n",
    "    preds = {key: [] for key in pipelines.keys()}\n",
    "    idx = x.index\n",
    "    for train, test in fit_ix_iter:\n",
    "        if test[-1] > len(x):\n",
    "            test = range(test[0], len(x))\n",
    "        temp_x = x.iloc[train, :]\n",
    "        cluster_indices = {'0': temp_x[temp_x['cluster_mode']==0].index, '1': temp_x[temp_x['cluster_mode']==1].index,\n",
    "                           '2': temp_x[temp_x['cluster_mode']==2].index, '3': temp_x[temp_x['cluster_mode']==3].index}\n",
    "\n",
    "        for key, value in pipelines.items():\n",
    "            value.fit(temp_x.loc[cluster_indices[key], filtered_vars[int(key)]], y.loc[cluster_indices[key]])\n",
    "            x_to_predict = x.iloc[test].loc[:, filtered_vars[int(key)]]\n",
    "            preds[key].append(pd.Series(data=value.predict(x_to_predict), index=x_to_predict.index))\n",
    "    \n",
    "    if export_model:\n",
    "        for key, value in pipelines.items():\n",
    "            joblib.dump(value, 'models/cluster_' + key + '_' + model_name + '.joblib')\n",
    "        \n",
    "    preds = pd.concat({k: pd.concat(v, axis=0) for k, v in preds.items()}, axis=1)\n",
    "    preds = preds[~preds.index.duplicated(keep='last')]\n",
    "    preds.columns = [str(col) + '_' + transformation + '_cluster' for col in preds.columns]\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_fit_predict_all(x, y, pipeline, cutoff_date, refit_freq, gap=7, export_model=True, model_name=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fits and predicts in a rolling basis\n",
    "    \"\"\"\n",
    "    \n",
    "    n_freq = {'D': 24, 'W': 24*7, 'M': 24*30}\n",
    "    refit_dates = pd.date_range(cutoff_date, x.index[-1], freq=refit_freq)\n",
    "    fit_ix_iter = zip(list(map(lambda z: range(x.index.get_loc(z)), refit_dates)), \n",
    "                      list(map(lambda z: range(x.index.get_loc(z) + gap, x.index.get_loc(z) + n_freq[refit_freq] + gap), refit_dates)))\n",
    "    \n",
    "    preds = []\n",
    "    idx = x.index\n",
    "    for train, test in fit_ix_iter:\n",
    "        if test[-1] > len(x):\n",
    "            test = range(test[0], len(x))\n",
    "        pipeline.fit(x.iloc[train].loc[:, filtered_vars['all']], y.iloc[train])\n",
    "        x_to_predict = x.iloc[test].loc[:, filtered_vars['all']]\n",
    "        preds.append(pd.Series(data=pipeline.predict(x_to_predict), index=x_to_predict.index))\n",
    "    \n",
    "    if export_model:\n",
    "        joblib.dump(pipeline, 'models/all_' + model_name + '.joblib')\n",
    "        \n",
    "    preds = pd.concat(preds, axis=0)\n",
    "    preds = preds[~preds.index.duplicated(keep='last')]\n",
    "    preds.name = transformation + '_all'\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will predict unseen data to see how the models perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n",
      "arsinh\n",
      "cuberoot\n"
     ]
    }
   ],
   "source": [
    "for transformation, function in data_transformations.items():\n",
    "    print(transformation)\n",
    "    temp_features = feats.copy()\n",
    "    temp_features.loc[:, ~temp_features.columns.isin(do_not_transform)] = temp_features.loc[:, ~temp_features.columns.isin(do_not_transform)].apply(function[0], axis=1) \n",
    "    lagged_feature = shift_dataset(temp_features.copy(), lag=True, forecast=False, nlag=50, dropna=True,\n",
    "                                  var_lags=vars_to_lag)\n",
    "    target = to_predict.loc[lagged_feature.index].apply(function[0])\n",
    "    oos_predictions[transformation + '_cluster'] = rolling_fit_predict_clusters(lagged_feature.copy(), target, best_pipelines_clusters[transformation], cutoff_date, 'D', export_model=True, model_name=transformation)\n",
    "    oos_predictions[transformation + '_all'] = rolling_fit_predict_all(lagged_feature.copy(), target, best_pipelines_all[transformation], cutoff_date, 'D', export_model=True, model_name=transformation)\n",
    "    pd.concat(oos_predictions.values(), axis=1).to_csv('temp_oos_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat(oos_predictions.values(), axis=1)\n",
    "preds_by_cluster = predictions.loc[:, list(map(lambda x: x.split('_')[-1] != 'all', predictions.columns))]\n",
    "preds_all = predictions.loc[:, list(map(lambda x: x.split('_')[-1] == 'all', predictions.columns))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have a prediction for each cluster and timestamp, we have to filter out the ones that do not correspond to the observed cluster in a given timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cluster_dict = {}\n",
    "for transformation in data_transformations.keys():\n",
    "    preds_by_cluster_temp = preds_by_cluster.loc[:, list(map(lambda x: x.split('_')[1] == transformation, preds_by_cluster.columns))]\n",
    "    preds_by_cluster_temp.columns = list(map(lambda x: int(x.split('_')[0]), preds_by_cluster_temp.columns))\n",
    "    preds_by_cluster_temp = pd.concat([preds_by_cluster_temp, feats.loc[:, 'cluster_mode']], axis=1)\n",
    "    preds_by_cluster_temp.dropna(inplace=True)\n",
    "    melted = preds_by_cluster_temp.melt(ignore_index=False, id_vars='cluster_mode')\n",
    "    melted = melted[melted['cluster_mode'] == melted['variable']]\n",
    "    preds_cluster_dict['bycluster_' + transformation] = melted.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cluster_df = pd.concat(preds_cluster_dict.values(), axis=1)\n",
    "preds_cluster_df.columns = preds_cluster_dict.keys()\n",
    "preds_output = pd.concat([preds_all, preds_cluster_df], axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output.to_csv('oos_preds.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
